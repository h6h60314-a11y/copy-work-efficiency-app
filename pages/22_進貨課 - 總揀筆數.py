# pages/22_é€²è²¨èª² - ç¸½æ€ç­†æ•¸.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
from io import BytesIO
from typing import List, Tuple

import numpy as np
import pandas as pd
import streamlit as st

from common_ui import inject_logistics_theme, set_page, card_open, card_close

pd.options.display.max_columns = 200

# éœ€è¦æ’é™¤çš„å„²ä½é—œéµå­—ï¼ˆå­å­—ä¸²æ¯”å°ï¼Œä¸åˆ†å¤§å°å¯«ï¼‰
EXCLUDE_SUBSTRINGS = ["CGS", "JCPL", "QC99", "GREAT0001X", "GX010", "PD99"]
EXCLUDE_PATTERN = re.compile("|".join(map(re.escape, EXCLUDE_SUBSTRINGS)), re.IGNORECASE)

# æ¨ç´#1 çš„åˆ—éšå±¤ï¼ˆä¸­ç¹¼ç”¨ï¼Œå‹•æ…‹æ“‡ç”¨ï¼‰
PIVOT1_BASE_ROWS = ["å„²ä½é¡å‹", "å„²ä½", "å•†å“"]  # ä¸€å®šæœƒç”¨
PIVOT1_OPTIONAL = ["æ€è²¨æ‰¹æ¬¡è™Ÿ"]               # è‹¥å­˜åœ¨æ‰ç”¨

# ä¾›æ¨ç´#1 å…§éƒ¨åŠ ç¸½ï¼ˆè‹¥æ¬„ä½å­˜åœ¨æ‰æœƒåˆè¨ˆï¼‰
NUM_COL_CANDIDATES = ["æ•¸é‡", "ä»¶æ•¸", "å‡ºè²¨é‡", "é…è²¨é‡", "æ‡‰æ€é‡", "RFæ€è²¨é‡", "å·®ç•°é‡"]

# è‹¥ä¾†æºæ²’æœ‰å•†å“æ¬„ä½ï¼Œè£œè™›æ“¬æ¬„ä½
PRODUCT_FALLBACK_COL = "å•†å“"


def normalize_loc(s):
    if pd.isna(s):
        return s
    return str(s).strip().upper()


def unit_mask_equal_2(series: pd.Series) -> pd.Series:
    """æˆç®±ï¼š=2ï¼ˆå­—é¢ '2' æˆ–æ•¸å€¼ 2/2.0ï¼‰"""
    s = series.astype(str).str.strip()
    mask_str = s.eq("2")
    s_num = pd.to_numeric(s, errors="coerce")
    mask_num = np.isfinite(s_num) & np.isclose(s_num, 2.0, rtol=0, atol=1e-9)
    return mask_str | mask_num


def unit_mask_contains_3_or_6(series: pd.Series) -> pd.Series:
    """é›¶æ•£ï¼šå­—ä¸²å« 3 æˆ– 6ï¼ˆå«å…¨å½¢ ï¼“ï¼ï¼–ï¼‰ï¼Œä»»æ„ä½ç½®"""
    s = series.astype(str)
    pat = re.compile(r"[3ï¼“]|[6ï¼–]")
    return s.str.contains(pat, na=False)


def _read_csv_auto(file_bytes: bytes) -> pd.DataFrame:
    last_err = None
    for enc in ("utf-8-sig", "utf-8", "cp950", "big5"):
        try:
            return pd.read_csv(BytesIO(file_bytes), encoding=enc, low_memory=False, dtype=str)
        except Exception as e:
            last_err = e
    raise RuntimeError(f"CSV/TXT è®€å–å¤±æ•—ï¼ˆå·²å˜—è©¦ utf-8-sig/utf-8/cp950/big5ï¼‰ï¼š{last_err}")


def read_excel_or_csv(uploaded) -> pd.DataFrame:
    """è®€å–®è¡¨ï¼ˆèˆ‡ä½ åŸæœ¬ read_excel è¡Œç‚ºä¸€è‡´ï¼‰ï¼Œæ”¯æ´ Excel/CSV/TXT"""
    name = uploaded.name
    _, ext = os.path.splitext(name)
    ext = ext.lower()
    b = uploaded.getvalue()

    if ext in (".csv", ".txt"):
        return _read_csv_auto(b)

    bio = BytesIO(b)
    if ext in (".xlsx", ".xlsm", ".xltx", ".xltm"):
        try:
            return pd.read_excel(bio, dtype=str, engine="openpyxl")
        except Exception:
            bio.seek(0)
            return pd.read_excel(bio, dtype=str)

    if ext == ".xls":
        try:
            return pd.read_excel(bio, dtype=str, engine="xlrd")
        except Exception as e:
            raise RuntimeError("ç›®å‰ç’°å¢ƒå¯èƒ½æœªå®‰è£ xlrdï¼Œ.xls ç„¡æ³•è®€å–ï¼›è«‹å…ˆå¦å­˜ç‚º .xlsx å†ä¸Šå‚³ã€‚") from e

    if ext == ".xlsb":
        try:
            return pd.read_excel(bio, dtype=str, engine="pyxlsb")
        except Exception as e:
            raise RuntimeError("ç›®å‰ç’°å¢ƒå¯èƒ½æœªå®‰è£ pyxlsbï¼Œ.xlsb ç„¡æ³•è®€å–ï¼›è«‹å…ˆå¦å­˜ç‚º .xlsx å†ä¸Šå‚³ã€‚") from e

    bio.seek(0)
    return pd.read_excel(bio, dtype=str)


def build_pivot2(df_source: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """
    å›å‚³ (#2 DataFrame, å¯¦éš›åˆ†çµ„éµæ¸…å–®)
    - è‹¥æœ‰ã€Œæ€è²¨æ‰¹æ¬¡è™Ÿã€ï¼š["å„²ä½é¡å‹","æ€è²¨æ‰¹æ¬¡è™Ÿ"]
    - è‹¥ç„¡ã€Œæ€è²¨æ‰¹æ¬¡è™Ÿã€ï¼š["å„²ä½é¡å‹"]
    """
    df_tmp = df_source.copy()

    # ä¾›ä¸­ç¹¼åˆè¨ˆä½¿ç”¨ï¼ˆä¸å½±éŸ¿ç­†æ•¸ï¼‰
    for c in [c for c in NUM_COL_CANDIDATES if c in df_tmp.columns]:
        df_tmp[c] = pd.to_numeric(df_tmp[c], errors="coerce")

    gb1 = [k for k in (PIVOT1_OPTIONAL + PIVOT1_BASE_ROWS) if k in df_tmp.columns]
    if ("å„²ä½é¡å‹" not in gb1) or ("å„²ä½" not in gb1):
        raise ValueError("æ¨ç´æ‰€éœ€æ¬„ä½ä¸è¶³ï¼Œè‡³å°‘è¦æœ‰ï¼šå„²ä½ã€å„²ä½é¡å‹ã€‚")

    # pivot1ï¼šä¸­ç¹¼ï¼ˆæŒ‰ å¯é¸æ‰¹æ¬¡è™Ÿ + å„²ä½é¡å‹ + å„²ä½ + å•†å“ï¼‰
    pivot1 = (
        df_tmp.groupby(gb1, dropna=False)
        .size()
        .reset_index(name="ç­†æ•¸")
    )

    group_keys = ["å„²ä½é¡å‹"] + (["æ€è²¨æ‰¹æ¬¡è™Ÿ"] if "æ€è²¨æ‰¹æ¬¡è™Ÿ" in pivot1.columns else [])

    # pivot2ï¼šè¼¸å‡ºï¼ˆæŒ‰ å„²ä½é¡å‹(+æ‰¹æ¬¡è™Ÿ) çµ±è¨ˆ å„²ä½ç­†æ•¸ï¼‰
    pivot2 = (
        pivot1.groupby(group_keys, dropna=False)["å„²ä½"]
        .count()
        .reset_index(name="å„²ä½_ç­†æ•¸")
        .sort_values(group_keys, kind="mergesort")
        .reset_index(drop=True)
    )
    pivot2["å„²ä½_ç­†æ•¸"] = pivot2["å„²ä½_ç­†æ•¸"].fillna(0).astype(int)
    return pivot2, group_keys


def process_subset(df_raw: pd.DataFrame, df_map: pd.DataFrame, subset_tag: str, mask: pd.Series):
    df_work = df_raw.loc[mask].copy()
    if df_work.empty:
        return subset_tag, None, 0, None

    # æ’é™¤å„²ä½
    df_work = df_work[~df_work["å„²ä½"].astype(str).str.contains(EXCLUDE_PATTERN, na=False)].copy()
    if df_work.empty:
        return subset_tag, None, 0, None

    # å›å¡«å„²ä½é¡å‹
    df_work["å„²ä½_norm"] = df_work["å„²ä½"].map(normalize_loc)
    map_first = (
        df_map.assign(å„²ä½_norm=df_map["å„²ä½"].map(normalize_loc))
        .sort_values(["å„²ä½_norm"])
        .drop_duplicates(subset=["å„²ä½_norm"], keep="first")[["å„²ä½_norm", "å„²ä½é¡å‹"]]
    )
    df_out = df_work.merge(map_first, on="å„²ä½_norm", how="left").drop(columns=["å„²ä½_norm"])

    pivot2, group_keys = build_pivot2(df_out)
    total_count = int(pivot2["å„²ä½_ç­†æ•¸"].sum(skipna=True))
    return subset_tag, pivot2, total_count, group_keys


def build_single_sheet_excel_bytes(df_type_total: pd.DataFrame, df_detail_all: pd.DataFrame, df_summary: pd.DataFrame) -> bytes:
    """
    å–®ä¸€å·¥ä½œè¡¨ï¼ˆä¾éœ€æ±‚é †åºï¼‰ï¼š
      1) ç¸½æ€ç­†æ•¸ï¼ˆæœ€ä¸Šæ–¹ï¼‰
      2) æ˜ç´°è¡¨ï¼ˆåˆä½µï¼‰
      3) å½™ç¸½ç¸½è¡¨
    """
    out = BytesIO()
    with pd.ExcelWriter(out, engine="openpyxl") as writer:
        sheet = "çµæœ"
        r = 0

        # 1) ç¸½æ€ç­†æ•¸ï¼ˆä¾å„²ä½é¡å‹åŠ ç¸½ï¼‰
        pd.DataFrame({"": ["ç¸½æ€ç­†æ•¸"]}).to_excel(
            writer, sheet_name=sheet, index=False, header=False, startrow=r, startcol=0
        )
        r += 1
        df_type_total.to_excel(writer, sheet_name=sheet, index=False, startrow=r, startcol=0)
        r += len(df_type_total) + 2

        # 2) æ˜ç´°è¡¨ï¼ˆåˆä½µï¼‰
        pd.DataFrame({"": ["æ˜ç´°è¡¨ï¼ˆåˆä½µï¼‰"]}).to_excel(
            writer, sheet_name=sheet, index=False, header=False, startrow=r, startcol=0
        )
        r += 1
        df_detail_all.to_excel(writer, sheet_name=sheet, index=False, startrow=r, startcol=0)
        r += len(df_detail_all) + 2

        # 3) å½™ç¸½ç¸½è¡¨
        pd.DataFrame({"": ["å½™ç¸½ç¸½è¡¨"]}).to_excel(
            writer, sheet_name=sheet, index=False, header=False, startrow=r, startcol=0
        )
        r += 1
        df_summary.to_excel(writer, sheet_name=sheet, index=False, startrow=r, startcol=0)

    out.seek(0)
    return out.read()


def _label_type_as_pick(type_name: str) -> str:
    t = str(type_name).strip()
    if t == "ä½ç©º":
        return "ä½ç©ºç¸½æ€ç­†æ•¸"
    if t == "é«˜ç©º":
        return "é«˜ç©ºç¸½æ€ç­†æ•¸"
    if t.upper() == "GM":
        return "GMç¸½æ€ç­†æ•¸"
    return f"{t}ç¸½æ€ç­†æ•¸"


def show_type_totals_as_text(df_type_total: pd.DataFrame):
    """âœ… ä¸ç”¨è¡¨æ ¼ï¼šç´”æ–‡å­—ç›´å‘é¡¯ç¤ºï¼ˆæ¨™é¡Œèˆ‡åç¨±ä¾éœ€æ±‚ï¼‰"""
    st.markdown("### ç¸½æ€ç­†æ•¸")
    if df_type_total is None or df_type_total.empty:
        st.caption("ï¼ˆç„¡è³‡æ–™ï¼‰")
        return

    for _, r in df_type_total.iterrows():
        t = r.get("å„²ä½é¡å‹", "")
        v = r.get("ç¸½æ€ç­†æ•¸", 0)
        st.markdown(f"**{_label_type_as_pick(t)}**")
        st.markdown(
            f"<div style='font-size:28px; font-weight:900; line-height:1.1; margin-top:2px; margin-bottom:14px;'>{int(v):,}</div>",
            unsafe_allow_html=True,
        )


# =========================
# UI
# =========================
inject_logistics_theme()
set_page("é€²è²¨èª²ï½œç¸½æ€ç­†æ•¸", icon="ğŸ¯", subtitle="å¤šæª”æ‰¹æ¬¡ï½œæˆç®±/é›¶æ•£ï¼ˆæˆ–ALLï¼‰ï½œæ’é™¤å„²ä½ï½œå›å¡«å„²ä½é¡å‹ï½œå–®é Excelè¼¸å‡º")

card_open("ğŸ¯ ç¸½æ€ç­†æ•¸ï¼ˆå–®é è¼¸å‡ºï¼‰")

batch_files = st.file_uploader(
    "ä¸Šå‚³ã€æ‰¹æ¬¡æ˜ç´°ã€‘ï¼ˆå¯å¤šæª”ï¼›è‡³å°‘å«æ¬„ä½ï¼šå„²ä½ï¼›æœ‰ã€è¨ˆé‡å–®ä½ã€æ›´ä½³ï¼‰",
    type=["xlsx", "xlsm", "xltx", "xltm", "xls", "xlsb", "csv", "txt"],
    accept_multiple_files=True,
)

map_file = st.file_uploader(
    "ä¸Šå‚³ã€å„²ä½æ£šåˆ¥æ˜ç´°ã€‘ï¼ˆéœ€å«æ¬„ä½ï¼šå„²ä½ã€å„²ä½é¡å‹ï¼‰",
    type=["xlsx", "xlsm", "xltx", "xltm", "xls", "xlsb", "csv", "txt"],
    accept_multiple_files=False,
)

st.markdown("---")

if (not batch_files) or (map_file is None):
    st.info("è«‹å…ˆä¸Šå‚³ã€Œæ‰¹æ¬¡æ˜ç´°ï¼ˆå¯å¤šæª”ï¼‰ã€èˆ‡ã€Œå„²ä½æ£šåˆ¥æ˜ç´°ã€ã€‚")
    card_close()
    st.stop()

run = st.button("é–‹å§‹ç”¢å‡º", type="primary")
if not run:
    card_close()
    st.stop()

# è®€å– map
try:
    df_map = read_excel_or_csv(map_file)
    df_map.columns = [str(c).strip() for c in df_map.columns]
except Exception as e:
    st.error(f"è®€å–ã€å„²ä½æ£šåˆ¥æ˜ç´°ã€å¤±æ•—ï¼š{e}")
    card_close()
    st.stop()

for c in ["å„²ä½", "å„²ä½é¡å‹"]:
    if c not in df_map.columns:
        st.error(f"å„²ä½æ£šåˆ¥æ˜ç´°ç¼ºå°‘æ¬„ä½ï¼š{c}")
        card_close()
        st.stop()

summary_rows: List[dict] = []
detail_frames: List[pd.DataFrame] = []
ok, fail = 0, 0

for up in batch_files:
    base = os.path.basename(up.name)
    name_noext = os.path.splitext(base)[0]

    try:
        df_raw = read_excel_or_csv(up)
        df_raw.columns = [str(c).strip() for c in df_raw.columns]
    except Exception:
        summary_rows.append({"ä¾†æºæª”å": name_noext, "å­é›†": "è®€æª”å¤±æ•—", "åˆ†çµ„éµ": "ç„¡", "åŠ ç¸½ç­†æ•¸": 0, "è³‡æ–™ç­†æ•¸": 0})
        fail += 1
        continue

    if "å„²ä½" not in df_raw.columns:
        summary_rows.append({"ä¾†æºæª”å": name_noext, "å­é›†": "ç¼ºæ¬„ä½", "åˆ†çµ„éµ": "ç„¡", "åŠ ç¸½ç­†æ•¸": 0, "è³‡æ–™ç­†æ•¸": 0})
        fail += 1
        continue

    if "å•†å“" not in df_raw.columns:
        df_raw[PRODUCT_FALLBACK_COL] = 1
        df_raw = df_raw.rename(columns={PRODUCT_FALLBACK_COL: "å•†å“"})

    has_unit_col = "è¨ˆé‡å–®ä½" in df_raw.columns
    masks = (
        [("æˆç®±", unit_mask_equal_2(df_raw["è¨ˆé‡å–®ä½"])),
         ("é›¶æ•£", unit_mask_contains_3_or_6(df_raw["è¨ˆé‡å–®ä½"]))]
        if has_unit_col
        else [("ALL", pd.Series([True] * len(df_raw), index=df_raw.index))]
    )

    any_ok = False
    for tag, mask in masks:
        tag, pivot2, total_count, group_keys = process_subset(df_raw, df_map, tag, mask)

        if pivot2 is None:
            summary_rows.append({"ä¾†æºæª”å": name_noext, "å­é›†": tag, "åˆ†çµ„éµ": "ç„¡", "åŠ ç¸½ç­†æ•¸": 0, "è³‡æ–™ç­†æ•¸": 0})
            continue

        grp_desc = " Ã— ".join(group_keys)

        df_detail = pivot2.copy()
        df_detail.insert(0, "ä¾†æºæª”å", name_noext)
        df_detail.insert(1, "å­é›†", tag)
        detail_frames.append(df_detail)

        summary_rows.append(
            {"ä¾†æºæª”å": name_noext, "å­é›†": tag, "åˆ†çµ„éµ": grp_desc, "åŠ ç¸½ç­†æ•¸": int(total_count), "è³‡æ–™ç­†æ•¸": int(len(pivot2))}
        )
        any_ok = True

    if any_ok:
        ok += 1
    else:
        fail += 1

df_summary = pd.DataFrame(summary_rows) if summary_rows else pd.DataFrame(columns=["ä¾†æºæª”å", "å­é›†", "åˆ†çµ„éµ", "åŠ ç¸½ç­†æ•¸", "è³‡æ–™ç­†æ•¸"])
if not df_summary.empty:
    df_summary = df_summary.sort_values(["ä¾†æºæª”å", "å­é›†"], kind="mergesort").reset_index(drop=True)

if detail_frames:
    df_detail_all = pd.concat(detail_frames, ignore_index=True)
    base_cols = ["ä¾†æºæª”å", "å­é›†", "å„²ä½é¡å‹"]
    cols = base_cols + (["æ€è²¨æ‰¹æ¬¡è™Ÿ"] if "æ€è²¨æ‰¹æ¬¡è™Ÿ" in df_detail_all.columns else []) + ["å„²ä½_ç­†æ•¸"]
    others = [c for c in df_detail_all.columns if c not in cols]
    df_detail_all = df_detail_all[cols + others]
else:
    df_detail_all = pd.DataFrame(columns=["ä¾†æºæª”å", "å­é›†", "å„²ä½é¡å‹", "å„²ä½_ç­†æ•¸"])

# âœ… ä¾å„²ä½é¡å‹åŠ ç¸½ã€Œç¸½æ€ç­†æ•¸ã€ï¼ˆå„²ä½_ç­†æ•¸ sumï¼‰
if (not df_detail_all.empty) and ("å„²ä½é¡å‹" in df_detail_all.columns) and ("å„²ä½_ç­†æ•¸" in df_detail_all.columns):
    df_type_total = (
        df_detail_all.groupby("å„²ä½é¡å‹", dropna=False)["å„²ä½_ç­†æ•¸"]
        .sum()
        .reset_index(name="ç¸½æ€ç­†æ•¸")
        .sort_values("ç¸½æ€ç­†æ•¸", ascending=False, kind="mergesort")
        .reset_index(drop=True)
    )
else:
    df_type_total = pd.DataFrame(columns=["å„²ä½é¡å‹", "ç¸½æ€ç­†æ•¸"])

# âœ… é¡¯ç¤ºï¼ˆä¸è¦è¡¨æ ¼ï¼‰
show_type_totals_as_text(df_type_total)

# âœ… ä½ è¦çš„é †åºï¼šæ˜ç´°è¡¨ï¼ˆåˆä½µï¼‰åœ¨å½™ç¸½ç¸½è¡¨ä¸Š
st.markdown("### æ˜ç´°è¡¨ï¼ˆåˆä½µï¼‰")
st.dataframe(df_detail_all, use_container_width=True, hide_index=True)

st.markdown("### å½™ç¸½ç¸½è¡¨")
st.dataframe(df_summary, use_container_width=True, hide_index=True)

st.caption(f"æˆåŠŸï¼š{ok} æª”ï¼›å¤±æ•—ï¼š{fail} æª”")

# ä¸‹è¼‰ï¼ˆExcelï¼šåŒä¸€å¼µå·¥ä½œè¡¨ï¼Œé †åºåŒç•«é¢ï¼‰
try:
    out_bytes = build_single_sheet_excel_bytes(df_type_total, df_detail_all, df_summary)
    out_name = "æ‰¹æ¬¡_ç¸½æ€ç­†æ•¸_å–®é è¼¸å‡º.xlsx"
except Exception as e:
    st.error(f"è¼¸å‡ºå¤±æ•—ï¼š{e}")
    card_close()
    st.stop()

st.download_button(
    "â¬‡ï¸ ä¸‹è¼‰è¼¸å‡º Excelï¼ˆå–®ä¸€å·¥ä½œè¡¨ï¼‰",
    data=out_bytes,
    file_name=out_name,
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

card_close()
