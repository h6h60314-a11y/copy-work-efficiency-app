# pages/10_é€²è²¨é©—æ”¶é‡.py
import io
import pandas as pd
import streamlit as st

from common_ui import inject_logistics_theme, set_page, card_open, card_close

st.set_page_config(page_title="é€²è²¨é©—æ”¶é‡ï½œå¤§æ¨¹KPI", page_icon="ðŸ“¥", layout="wide")
inject_logistics_theme()

SHEET_DEFAULT = "æŽ¡è³¼å–®é©—æ”¶é‡æ˜Žç´°"
REQ_COLS = ["å…¥åº«é¡žåž‹", "é©—æ”¶å…¥åº«æ•¸é‡", "ä¾›æ‡‰å•†ä»£è™Ÿ", "DCæŽ¡è³¼å–®è™Ÿ", "å•†å“å“è™Ÿ"]


def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df


def _is_empty_row(vals) -> bool:
    for v in vals:
        if v is None:
            continue
        if isinstance(v, str) and v.strip() == "":
            continue
        return False
    return True


def _coerce_str(v):
    if v is None:
        return ""
    return str(v).strip()


def _trim_trailing_nones(vals):
    last = -1
    for i, v in enumerate(vals):
        if v is None:
            continue
        if isinstance(v, str) and v.strip() == "":
            continue
        last = i
    if last < 0:
        return []
    return vals[: last + 1]


def _find_header_row(rows, required_cols, scan_rows=200):
    """
    åœ¨å‰ scan_rows è¡Œå…§ï¼Œæ‰¾å‡ºåŒ…å«å¿…è¦æ¬„ä½çš„è¡¨é ­åˆ—
    """
    req = set([c.strip() for c in required_cols])
    for i, r in enumerate(rows[:scan_rows]):
        cand = [_coerce_str(x) for x in r]
        cand_set = set([x for x in cand if x])
        # å®Œæ•´å‘½ä¸­æœ€å¥½ï¼›è‹¥æª”æ¡ˆæ¬„ä½å¤šä¸€é»ž/é †åºä¸åŒä¹Ÿèƒ½æŠ“åˆ°
        if req.issubset(cand_set):
            return i
    return None


def _read_xlsb_with_pyxlsb(file_bytes: bytes, sheet_name: str) -> pd.DataFrame:
    """
    ç›´æŽ¥ç”¨ pyxlsb è§£æž .xlsbï¼Œé¿å… pandas å° pyxlsb ç‰ˆæœ¬è¦æ±‚å•é¡Œ
    """
    try:
        from pyxlsb import open_workbook
    except Exception as e:
        raise ImportError("è®€å– .xlsb éœ€è¦å®‰è£ pyxlsbã€‚è«‹åœ¨ requirements.txt åŠ ä¸Šï¼špyxlsb") from e

    bio = io.BytesIO(file_bytes)
    rows = []

    with open_workbook(bio) as wb:
        if sheet_name not in wb.sheets:
            raise KeyError(f"æ‰¾ä¸åˆ°å·¥ä½œè¡¨ï¼š{sheet_name}ï¼ˆç›®å‰å·¥ä½œè¡¨ï¼š{', '.join(wb.sheets)}ï¼‰")

        with wb.get_sheet(sheet_name) as sh:
            for row in sh.rows():
                vals = [c.v for c in row]
                vals = _trim_trailing_nones(vals)
                rows.append(vals)

    if not rows:
        return pd.DataFrame()

    # æ‰¾è¡¨é ­åˆ—ï¼ˆå„ªå…ˆæ‰¾åŒ…å«å¿…è¦æ¬„ä½çš„é‚£ä¸€åˆ—ï¼‰
    header_idx = _find_header_row(rows, REQ_COLS, scan_rows=250)
    if header_idx is None:
        # é€€è€Œæ±‚å…¶æ¬¡ï¼šæ‰¾ç¬¬ä¸€å€‹éžç©ºåˆ—ç•¶ header
        header_idx = next((i for i, r in enumerate(rows[:250]) if not _is_empty_row(r)), None)

    if header_idx is None:
        return pd.DataFrame()

    header = [str(x).strip() if x is not None else "" for x in rows[header_idx]]
    header = [h if h else f"æœªå‘½åæ¬„ä½_{i+1}" for i, h in enumerate(header)]

    data_rows = rows[header_idx + 1 :]

    # åŽ»æŽ‰å‰é¢ä¸€å †ç©ºç™½åˆ—
    while data_rows and _is_empty_row(data_rows[0]):
        data_rows = data_rows[1:]

    # åœåœ¨é€£çºŒç©ºç™½åˆ—å¤ªå¤šï¼ˆé¿å…æ•´å¼µè¡¨å°¾ç«¯ç©ºåˆ°çˆ†ï¼‰
    cleaned = []
    empty_run = 0
    for r in data_rows:
        if _is_empty_row(r):
            empty_run += 1
            if empty_run >= 30:
                break
            continue
        empty_run = 0
        cleaned.append(r)

    if not cleaned:
        return pd.DataFrame(columns=header)

    # è£œé½Šæ¬„ä½é•·åº¦
    max_len = len(header)
    fixed = []
    for r in cleaned:
        rr = list(r[:max_len]) + [None] * max(0, max_len - len(r))
        fixed.append(rr)

    df = pd.DataFrame(fixed, columns=header)
    return _normalize_cols(df)


def _get_sheet_names(file_name: str, file_bytes: bytes):
    ext = file_name.lower().split(".")[-1]
    bio = io.BytesIO(file_bytes)

    if ext == "xlsb":
        from pyxlsb import open_workbook

        with open_workbook(bio) as wb:
            return list(wb.sheets)

    # xlsx / xlsm
    try:
        from openpyxl import load_workbook
    except Exception as e:
        raise ImportError("è®€å– xlsx/xlsm éœ€è¦ openpyxlã€‚") from e

    wb = load_workbook(bio, read_only=True, data_only=True)
    return wb.sheetnames


@st.cache_data(show_spinner=False)
def _read_excel_bytes(file_name: str, file_bytes: bytes, sheet_name: str) -> pd.DataFrame:
    ext = file_name.lower().split(".")[-1]
    bio = io.BytesIO(file_bytes)

    if ext == "xlsb":
        return _read_xlsb_with_pyxlsb(file_bytes, sheet_name)

    # xlsx / xlsmï¼šç”¨ openpyxl
    df = pd.read_excel(bio, sheet_name=sheet_name, engine="openpyxl")
    return _normalize_cols(df)


def _compute_stats(df: pd.DataFrame, inbound_type: str) -> dict:
    df = df.copy()

    for c in REQ_COLS:
        if c not in df.columns:
            raise KeyError(f"æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼š{c}")

    df_type = df[df["å…¥åº«é¡žåž‹"].astype(str).str.strip().eq(inbound_type)].copy()
    df_type["é©—æ”¶å…¥åº«æ•¸é‡"] = pd.to_numeric(df_type["é©—æ”¶å…¥åº«æ•¸é‡"], errors="coerce").fillna(0)

    return {
        "type": inbound_type,
        "unique_suppliers": int(df_type["ä¾›æ‡‰å•†ä»£è™Ÿ"].nunique(dropna=True)),
        "unique_dc_orders": int(df_type["DCæŽ¡è³¼å–®è™Ÿ"].nunique(dropna=True)),
        "unique_products": int(df_type["å•†å“å“è™Ÿ"].nunique(dropna=True)),
        "total_qty": float(df_type["é©—æ”¶å…¥åº«æ•¸é‡"].sum()),
    }


def _fmt_qty(x: float) -> str:
    if abs(x - round(x)) < 1e-9:
        return f"{int(round(x)):,}"
    return f"{x:,.2f}"


def main():
    set_page(
        "é€²è²¨é©—æ”¶é‡",
        icon="ðŸ“¥",
        subtitle="å¤§æ¨¹KPIï½œæ¯æ—¥é©—æ”¶å ±è¡¨ï½œGPO / GXPO çµ±è¨ˆ",
    )

    card_open("ðŸ“Œ ä¸Šå‚³æª”æ¡ˆ")
    up = st.file_uploader("ä¸Šå‚³ .xlsb æˆ– .xlsx", type=["xlsb", "xlsx", "xlsm"])
    card_close()

    if not up:
        st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆå¾Œå†åŸ·è¡Œçµ±è¨ˆã€‚")
        return

    file_bytes = up.getvalue()

    # âœ… è®€å–å·¥ä½œè¡¨æ¸…å–® â†’ ä¸‹æ‹‰é¸æ“‡
    try:
        sheet_names = _get_sheet_names(up.name, file_bytes)
    except Exception as e:
        st.error(f"è®€å–å·¥ä½œè¡¨æ¸…å–®å¤±æ•—ï¼š{e}")
        st.stop()

    default_idx = 0
    if SHEET_DEFAULT in sheet_names:
        default_idx = sheet_names.index(SHEET_DEFAULT)

    card_open("âš™ï¸ è®€å–è¨­å®š")
    sheet_name = st.selectbox("å·¥ä½œè¡¨åç¨±", options=sheet_names, index=default_idx)
    card_close()

    with st.spinner("è®€å–è³‡æ–™ä¸­..."):
        try:
            df = _read_excel_bytes(up.name, file_bytes, sheet_name)
        except Exception as e:
            st.error(f"è®€å–å¤±æ•—ï¼š{e}")
            st.stop()

    if df.empty:
        st.warning(
            "å·²æˆåŠŸè®€å–æª”æ¡ˆï¼Œä½†é€™å¼µå·¥ä½œè¡¨è³‡æ–™æ˜¯ç©ºçš„æˆ–æ²’æœ‰å¯è§£æžçš„å€¼ã€‚\n\n"
            "è‹¥ä½ é€™ä»½ .xlsb æ˜¯ Excel å…¬å¼/æ¨žç´å³æ™‚ç”¢ç”Ÿçš„å ±è¡¨ï¼š\n"
            "å»ºè­°å…ˆç”¨ Excel é–‹å•Ÿä¸€æ¬¡ â†’ è®“å®ƒè¨ˆç®—å®Œæˆ â†’ å¦å­˜æ–°æª”ç‚º .xlsx å†ä¸Šå‚³ã€‚"
        )
        st.write("ç›®å‰æ¬„ä½ï¼š", list(df.columns))
        return

    # åŸºæœ¬æª¢æŸ¥
    missing = [c for c in REQ_COLS if c not in df.columns]
    if missing:
        st.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{', '.join(missing)}")
        st.write("ç›®å‰æ¬„ä½ï¼š", list(df.columns))
        st.stop()

    # çµ±è¨ˆ
    types = ["GPO", "GXPO"]
    stats_rows = []
    for t in types:
        stats_rows.append(_compute_stats(df, t))

    overall_unique_suppliers = int(df["ä¾›æ‡‰å•†ä»£è™Ÿ"].nunique(dropna=True))

    # é¡¯ç¤ºçµæžœ
    card_open("ðŸ“Š çµ±è¨ˆçµæžœ")
    cols = st.columns(len(types))
    for i, s in enumerate(stats_rows):
        with cols[i]:
            st.subheader(f"{s['type']} é¡žåž‹")
            st.metric("ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿ", f"{s['unique_suppliers']:,} ç­†")
            st.metric("ä¸é‡è¤‡ DCæŽ¡è³¼å–®è™Ÿ", f"{s['unique_dc_orders']:,} ç­†")
            st.metric("ä¸é‡è¤‡ å•†å“å“è™Ÿ", f"{s['unique_products']:,} ç­†")
            st.metric("é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡", _fmt_qty(s["total_qty"]))

    st.divider()
    st.metric("ç¸½æ˜Žç´°ï¼šä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿç¸½æ•¸", f"{overall_unique_suppliers:,} ç­†")
    card_close()

    # åŒ¯å‡º
    out_df = pd.DataFrame(
        [
            {
                "å…¥åº«é¡žåž‹": r["type"],
                "ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿæ•¸": r["unique_suppliers"],
                "ä¸é‡è¤‡DCæŽ¡è³¼å–®è™Ÿæ•¸": r["unique_dc_orders"],
                "ä¸é‡è¤‡å•†å“å“è™Ÿæ•¸": r["unique_products"],
                "é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡": r["total_qty"],
            }
            for r in stats_rows
        ]
    )
    out_df.loc[len(out_df)] = {
        "å…¥åº«é¡žåž‹": "ç¸½æ˜Žç´°",
        "ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿæ•¸": overall_unique_suppliers,
        "ä¸é‡è¤‡DCæŽ¡è³¼å–®è™Ÿæ•¸": None,
        "ä¸é‡è¤‡å•†å“å“è™Ÿæ•¸": None,
        "é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡": None,
    }

    card_open("ðŸ“¤ åŒ¯å‡º")
    st.dataframe(out_df, use_container_width=True)

    csv_bytes = out_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("ä¸‹è¼‰ CSV", data=csv_bytes, file_name="é€²è²¨é©—æ”¶é‡_çµ±è¨ˆ.csv", mime="text/csv")
    card_close()


if __name__ == "__main__":
    main()
