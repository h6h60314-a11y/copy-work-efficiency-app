# pages/10_é€²è²¨é©—æ”¶é‡.py
import io
import re
import pandas as pd
import streamlit as st

from common_ui import inject_logistics_theme, set_page, card_open, card_close

st.set_page_config(page_title="é€²è²¨é©—æ”¶é‡ï½œå¤§æ¨¹KPI", page_icon="ðŸ“¥", layout="wide")
inject_logistics_theme()

SHEET_DEFAULT = "æŽ¡è³¼å–®é©—æ”¶é‡æ˜Žç´°"

# âœ… çµ±ä¸€ç”¨ã€Œæ¨™æº–æ¬„ä½åã€åšé‹ç®—ï¼ˆå…ˆæŠŠå ±è¡¨æ¬„ä½è‡ªå‹•å°ç…§åˆ°é€™äº›åç¨±ï¼‰
REQ_COLS = ["å…¥åº«é¡žåž‹", "é©—æ”¶å…¥åº«æ•¸é‡", "ä¾›æ‡‰å•†ä»£è™Ÿ", "DCæŽ¡è³¼å–®è™Ÿ", "å•†å“å“è™Ÿ"]

# âœ… ä½ çš„æª”æ¡ˆå¯¦éš›æœƒå‡ºç¾çš„åŒç¾©æ¬„ä½ï¼ˆå¯å†åŠ ï¼‰
COL_ALIASES = {
    "å…¥åº«é¡žåž‹": ["å…¥åº«é¡žåž‹", "å…¥åº«åž‹æ…‹", "å…¥åº«ç±»åž‹"],
    "é©—æ”¶å…¥åº«æ•¸é‡": ["é©—æ”¶å…¥åº«æ•¸é‡", "é©—æ”¶å…¥åº«é‡", "é©—æ”¶å…¥åº«", "é©—æ”¶å…¥åº«é‡", "é©—æ”¶å…¥åº«æ•¸é‡"],
    "ä¾›æ‡‰å•†ä»£è™Ÿ": ["ä¾›æ‡‰å•†ä»£è™Ÿ", "å» å•†ä»£è™Ÿ", "ä¾›æ‡‰å•†ç·¨è™Ÿ", "å» å•†ç·¨è™Ÿ"],
    "DCæŽ¡è³¼å–®è™Ÿ": ["DCæŽ¡è³¼å–®è™Ÿ", "DCæŽ¡è³¼å–®å·", "DCæŽ¡è³¼å–®", "æŽ¡è³¼å–®è™Ÿ(DC)"],
    # âš ï¸ è‹¥çœŸçš„æ²’æœ‰ DCæŽ¡è³¼å–®è™Ÿï¼Œæ‰é€€å›žæŽ¡è³¼å–®è™Ÿ
    "__FALLBACK_DC_ORDER__": ["æŽ¡è³¼å–®è™Ÿ"],
    "å•†å“å“è™Ÿ": ["å•†å“å“è™Ÿ", "å•†å“ä»£è™Ÿ", "å•†å“æ–™è™Ÿ", "å“è™Ÿ", "æ–™è™Ÿ"],
}


def _norm_key(s: str) -> str:
    # åŽ»ç©ºç™½ã€å…¨å½¢ç©ºç™½ã€å¸¸è¦‹ç¬¦è™Ÿ
    s = str(s)
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", "", s)
    return s.strip()


def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().replace("\u3000", " ") for c in df.columns]
    return df


def _apply_column_aliases(df: pd.DataFrame) -> pd.DataFrame:
    """
    æŠŠã€Œå ±è¡¨æ¬„ä½ã€è‡ªå‹•å°ç…§æˆã€Œæ¨™æº–æ¬„ä½ã€(REQ_COLS)
    - åªåœ¨æ¨™æº–æ¬„ä½ä¸å­˜åœ¨æ™‚æ‰æœƒåš rename
    - DCæŽ¡è³¼å–®è™Ÿï¼šè‹¥æ²’æœ‰æ‰ç”¨ æŽ¡è³¼å–®è™Ÿ ç•¶ fallback
    """
    df = df.copy()
    cols = list(df.columns)

    # å»ºä¸€å€‹ normalized -> åŽŸå§‹æ¬„ä½å çš„ lookup
    norm_map = {}
    for c in cols:
        nk = _norm_key(c)
        if nk not in norm_map:
            norm_map[nk] = c

    def find_col(candidates):
        for cand in candidates:
            # å…ˆç›´æŽ¥å‘½ä¸­
            if cand in df.columns:
                return cand
            # å†ç”¨ normalized å‘½ä¸­
            nk = _norm_key(cand)
            if nk in norm_map:
                return norm_map[nk]
        return None

    rename_map = {}

    # ä¸€èˆ¬æ¬„ä½
    for target, candidates in COL_ALIASES.items():
        if target.startswith("__"):
            continue
        if target in df.columns:
            continue
        hit = find_col(candidates)
        if hit and hit != target:
            rename_map[hit] = target

    # DCæŽ¡è³¼å–®è™Ÿ fallbackï¼ˆåªæœ‰åœ¨çœŸçš„æ²’æœ‰ DCæŽ¡è³¼å–®è™Ÿ æ™‚æ‰ç”¨ã€ŒæŽ¡è³¼å–®è™Ÿã€ï¼‰
    if "DCæŽ¡è³¼å–®è™Ÿ" not in df.columns:
        fb = find_col(COL_ALIASES["__FALLBACK_DC_ORDER__"])
        if fb:
            rename_map[fb] = "DCæŽ¡è³¼å–®è™Ÿ"

    if rename_map:
        df = df.rename(columns=rename_map)

    return df


def _is_empty_row(vals) -> bool:
    for v in vals:
        if v is None:
            continue
        if isinstance(v, str) and v.strip() == "":
            continue
        return False
    return True


def _trim_trailing_nones(vals):
    last = -1
    for i, v in enumerate(vals):
        if v is None:
            continue
        if isinstance(v, str) and v.strip() == "":
            continue
        last = i
    if last < 0:
        return []
    return vals[: last + 1]


def _find_header_row(rows, required_cols, scan_rows=250):
    req = set([c.strip() for c in required_cols])
    for i, r in enumerate(rows[:scan_rows]):
        cand = [str(x).strip() if x is not None else "" for x in r]
        cand = [c for c in cand if c]
        cand_set = set(cand)
        if req.issubset(cand_set):
            return i
    return None


def _read_xlsb_with_pyxlsb(file_bytes: bytes, sheet_name: str) -> pd.DataFrame:
    try:
        from pyxlsb import open_workbook
    except Exception as e:
        raise ImportError("è®€å– .xlsb éœ€è¦å®‰è£ pyxlsbï¼ˆrequirements.txt åŠ ä¸Š pyxlsbï¼‰ã€‚") from e

    bio = io.BytesIO(file_bytes)
    rows = []

    with open_workbook(bio) as wb:
        if sheet_name not in wb.sheets:
            raise KeyError(f"æ‰¾ä¸åˆ°å·¥ä½œè¡¨ï¼š{sheet_name}ï¼ˆç›®å‰å·¥ä½œè¡¨ï¼š{', '.join(wb.sheets)}ï¼‰")

        with wb.get_sheet(sheet_name) as sh:
            for row in sh.rows():
                vals = [c.v for c in row]
                vals = _trim_trailing_nones(vals)
                rows.append(vals)

    if not rows:
        return pd.DataFrame()

    # å…ˆç”¨ã€Œæ¨™æº–æ¬„ä½ã€æ‰¾è¡¨é ­ï¼›æ‰¾ä¸åˆ°å°±é€€å›žç¬¬ä¸€å€‹éžç©ºåˆ—
    header_idx = _find_header_row(rows, list(set(sum([v for k, v in COL_ALIASES.items() if not k.startswith("__")], []))), scan_rows=250)
    if header_idx is None:
        header_idx = next((i for i, r in enumerate(rows[:250]) if not _is_empty_row(r)), None)

    if header_idx is None:
        return pd.DataFrame()

    header = [str(x).strip() if x is not None else "" for x in rows[header_idx]]
    header = [h if h else f"æœªå‘½åæ¬„ä½_{i+1}" for i, h in enumerate(header)]

    data_rows = rows[header_idx + 1 :]

    # åŽ»æŽ‰å‰é¢ç©ºç™½åˆ—
    while data_rows and _is_empty_row(data_rows[0]):
        data_rows = data_rows[1:]

    cleaned = []
    empty_run = 0
    for r in data_rows:
        if _is_empty_row(r):
            empty_run += 1
            if empty_run >= 30:
                break
            continue
        empty_run = 0
        cleaned.append(r)

    if not cleaned:
        return pd.DataFrame(columns=header)

    max_len = len(header)
    fixed = []
    for r in cleaned:
        rr = list(r[:max_len]) + [None] * max(0, max_len - len(r))
        fixed.append(rr)

    df = pd.DataFrame(fixed, columns=header)
    df = _normalize_cols(df)
    df = _apply_column_aliases(df)
    return df


def _get_sheet_names(file_name: str, file_bytes: bytes):
    ext = file_name.lower().split(".")[-1]
    bio = io.BytesIO(file_bytes)

    if ext == "xlsb":
        from pyxlsb import open_workbook
        with open_workbook(bio) as wb:
            return list(wb.sheets)

    from openpyxl import load_workbook
    wb = load_workbook(bio, read_only=True, data_only=True)
    return wb.sheetnames


@st.cache_data(show_spinner=False)
def _read_excel_bytes(file_name: str, file_bytes: bytes, sheet_name: str) -> pd.DataFrame:
    ext = file_name.lower().split(".")[-1]
    bio = io.BytesIO(file_bytes)

    if ext == "xlsb":
        return _read_xlsb_with_pyxlsb(file_bytes, sheet_name)

    # xlsx / xlsm
    df = pd.read_excel(bio, sheet_name=sheet_name, engine="openpyxl")
    df = _normalize_cols(df)
    df = _apply_column_aliases(df)
    return df


def _compute_stats(df: pd.DataFrame, inbound_type: str) -> dict:
    df = df.copy()

    # æ¬„ä½ä¿è­·
    for c in REQ_COLS:
        if c not in df.columns:
            raise KeyError(f"æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼š{c}")

    df_type = df[df["å…¥åº«é¡žåž‹"].astype(str).str.strip().eq(inbound_type)].copy()
    df_type["é©—æ”¶å…¥åº«æ•¸é‡"] = pd.to_numeric(df_type["é©—æ”¶å…¥åº«æ•¸é‡"], errors="coerce").fillna(0)

    return {
        "type": inbound_type,
        "unique_suppliers": int(df_type["ä¾›æ‡‰å•†ä»£è™Ÿ"].nunique(dropna=True)),
        "unique_dc_orders": int(df_type["DCæŽ¡è³¼å–®è™Ÿ"].nunique(dropna=True)),
        "unique_products": int(df_type["å•†å“å“è™Ÿ"].nunique(dropna=True)),
        "total_qty": float(df_type["é©—æ”¶å…¥åº«æ•¸é‡"].sum()),
    }


def _fmt_qty(x: float) -> str:
    if abs(x - round(x)) < 1e-9:
        return f"{int(round(x)):,}"
    return f"{x:,.2f}"


def main():
    set_page(
        "é€²è²¨é©—æ”¶é‡",
        icon="ðŸ“¥",
        subtitle="å¤§æ¨¹KPIï½œæ¯æ—¥é©—æ”¶å ±è¡¨ï½œGPO / GXPO çµ±è¨ˆ",
    )

    card_open("ðŸ“Œ ä¸Šå‚³æª”æ¡ˆ")
    up = st.file_uploader("ä¸Šå‚³ .xlsb æˆ– .xlsx", type=["xlsb", "xlsx", "xlsm"])
    card_close()

    if not up:
        st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆå¾Œå†åŸ·è¡Œçµ±è¨ˆã€‚")
        return

    file_bytes = up.getvalue()

    # å·¥ä½œè¡¨æ¸…å–®
    try:
        sheet_names = _get_sheet_names(up.name, file_bytes)
    except Exception as e:
        st.error(f"è®€å–å·¥ä½œè¡¨æ¸…å–®å¤±æ•—ï¼š{e}")
        st.stop()

    default_idx = sheet_names.index(SHEET_DEFAULT) if SHEET_DEFAULT in sheet_names else 0

    card_open("âš™ï¸ è®€å–è¨­å®š")
    sheet_name = st.selectbox("å·¥ä½œè¡¨åç¨±", options=sheet_names, index=default_idx)
    card_close()

    with st.spinner("è®€å–è³‡æ–™ä¸­..."):
        try:
            df = _read_excel_bytes(up.name, file_bytes, sheet_name)
        except Exception as e:
            st.error(f"è®€å–å¤±æ•—ï¼š{e}")
            st.stop()

    if df.empty:
        st.warning(
            "å·²æˆåŠŸè®€å–æª”æ¡ˆï¼Œä½†é€™å¼µå·¥ä½œè¡¨è³‡æ–™æ˜¯ç©ºçš„æˆ–æ²’æœ‰å¯è§£æžçš„å€¼ã€‚\n\n"
            "è‹¥é€™ä»½å ±è¡¨æ˜¯å…¬å¼/æ¨žç´å³æ™‚ç”¢ç”Ÿï¼šå»ºè­°å…ˆç”¨ Excel é–‹å•Ÿä¸€æ¬¡ â†’ ç­‰è¨ˆç®—å®Œæˆ â†’ å¦å­˜ç‚º .xlsx å†ä¸Šå‚³ã€‚"
        )
        st.write("ç›®å‰æ¬„ä½ï¼š", list(df.columns))
        return

    # âœ… é¡¯ç¤ºã€Œæ¬„ä½å°ç…§å¾Œã€çš„æ¬„åï¼Œæ–¹ä¾¿ä½ ç¢ºèª
    with st.expander("ðŸ”Ž æª¢è¦–ç›®å‰æ¬„ä½ï¼ˆå·²è‡ªå‹•å°ç…§ï¼‰", expanded=False):
        st.write(list(df.columns))

    # å¿…è¦æ¬„ä½æª¢æŸ¥
    missing = [c for c in REQ_COLS if c not in df.columns]
    if missing:
        st.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{', '.join(missing)}")
        st.write("ç›®å‰æ¬„ä½ï¼š", list(df.columns))
        st.stop()

    # çµ±è¨ˆ
    types = ["GPO", "GXPO"]
    stats_rows = []
    for t in types:
        stats_rows.append(_compute_stats(df, t))

    overall_unique_suppliers = int(df["ä¾›æ‡‰å•†ä»£è™Ÿ"].nunique(dropna=True))

    card_open("ðŸ“Š çµ±è¨ˆçµæžœ")
    cols = st.columns(len(types))
    for i, s in enumerate(stats_rows):
        with cols[i]:
            st.subheader(f"{s['type']} é¡žåž‹")
            st.metric("ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿ", f"{s['unique_suppliers']:,} ç­†")
            st.metric("ä¸é‡è¤‡ DCæŽ¡è³¼å–®è™Ÿ", f"{s['unique_dc_orders']:,} ç­†")
            st.metric("ä¸é‡è¤‡ å•†å“å“è™Ÿ", f"{s['unique_products']:,} ç­†")
            st.metric("é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡", _fmt_qty(s["total_qty"]))

    st.divider()
    st.metric("ç¸½æ˜Žç´°ï¼šä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿç¸½æ•¸", f"{overall_unique_suppliers:,} ç­†")
    card_close()

    # åŒ¯å‡º
    out_df = pd.DataFrame(
        [
            {
                "å…¥åº«é¡žåž‹": r["type"],
                "ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿæ•¸": r["unique_suppliers"],
                "ä¸é‡è¤‡DCæŽ¡è³¼å–®è™Ÿæ•¸": r["unique_dc_orders"],
                "ä¸é‡è¤‡å•†å“å“è™Ÿæ•¸": r["unique_products"],
                "é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡": r["total_qty"],
            }
            for r in stats_rows
        ]
    )
    out_df.loc[len(out_df)] = {
        "å…¥åº«é¡žåž‹": "ç¸½æ˜Žç´°",
        "ä¸é‡è¤‡ä¾›æ‡‰å•†ä»£è™Ÿæ•¸": overall_unique_suppliers,
        "ä¸é‡è¤‡DCæŽ¡è³¼å–®è™Ÿæ•¸": None,
        "ä¸é‡è¤‡å•†å“å“è™Ÿæ•¸": None,
        "é©—æ”¶å…¥åº«æ•¸é‡ç¸½é‡": None,
    }

    card_open("ðŸ“¤ åŒ¯å‡º")
    st.dataframe(out_df, use_container_width=True)

    csv_bytes = out_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("ä¸‹è¼‰ CSV", data=csv_bytes, file_name="é€²è²¨é©—æ”¶é‡_çµ±è¨ˆ.csv", mime="text/csv")
    card_close()


if __name__ == "__main__":
    main()
