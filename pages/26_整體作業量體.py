# pages/26_æ•´é«”ä½œæ¥­é‡é«”.py
# -*- coding: utf-8 -*-
from __future__ import annotations

from io import BytesIO, StringIO
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st

from common_ui import (
    inject_logistics_theme,
    set_page,
    KPI,
    render_kpis,
    card_open,
    card_close,
    download_excel_card,  # âœ… ä¸€è¡Œ=æŒ‰éˆ•ï¼ˆä¸”å¤–æ¡†ä¸åˆ†æ®µï¼‰
)

st.set_page_config(page_title="å¤§è±KPIï½œæ•´é«”ä½œæ¥­é‡é«”", page_icon="ğŸ§¹", layout="wide")
inject_logistics_theme()

set_page(
    "æ•´é«”ä½œæ¥­é‡é«”",
    icon="ğŸ§¹",
    subtitle="æ”¯æ´ Excel/TXTï½œå¯å¤šæª”ä¸Šå‚³ï½œåˆªé™¤ç®±é¡å‹å«ã€ç«™æ‰€ã€ï½œè¨ˆé‡å–®ä½æ•¸é‡ï½œå‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰ï½œGM/ä¸€èˆ¬å€‰ Ã— æˆç®±/é›¶æ•£çµ±è¨ˆï½œExcelä¸‹è¼‰",
)

# ----------------------------
# constants / helpers
# ----------------------------
NEED_COLS = ["packqty", "å…¥æ•¸", "ç®±é¡å‹", "è¼‰å…·è™Ÿ", "BOXTYPE", "boxid"]
CANDIDATE_SEPS = ["\t", ",", "|", ";"]
CANDIDATE_ENCODINGS = ["utf-8-sig", "utf-8", "cp950", "big5", "latin1"]  # latin1 æœ€å¾Œå…œåº•


def _safe_str(s: pd.Series) -> pd.Series:
    return s.astype(str).fillna("").astype(str)


def _fmt_int(x) -> str:
    try:
        return f"{int(round(float(x))):,}"
    except Exception:
        return "0"


def _fmt0(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return "0"


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df


def _detect_sep(text: str) -> str | None:
    """
    ç²—ç•¥çŒœåˆ†éš”ç¬¦ï¼š
    - è‹¥ Tab/é€—è™Ÿ/|/; éƒ½æ²’æœ‰æ˜é¡¯å‡ºç¾ï¼Œå›å‚³ Noneï¼ˆä»£è¡¨å¯èƒ½æ˜¯å¤šç©ºç™½/å›ºå®šå¯¬åº¦ï¼‰
    """
    lines = [ln for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None
    first = lines[0]

    best = None
    best_cnt = 0
    for sep in CANDIDATE_SEPS:
        cnt = first.count(sep)
        if cnt > best_cnt:
            best_cnt = cnt
            best = sep

    # æ²’æœ‰åˆ†éš”ç¬¦
    if best_cnt <= 0:
        return None
    return best


def _read_txt_as_df(text: str, sep: str | None) -> pd.DataFrame:
    """
    ä¾ sep è®€å–ï¼š
    - sep=None -> å…ˆå˜—è©¦å¤šç©ºç™½(r'\s+')ï¼Œå†å˜—è©¦å›ºå®šå¯¬åº¦ read_fwf
    """
    # 1) æ˜ç¢ºåˆ†éš”ç¬¦
    if sep is not None and sep != "__fwf__":
        return pd.read_csv(
            StringIO(text),
            sep=sep,
            dtype=str,
            engine="python",
        )

    # 2) å¤šç©ºç™½åˆ†æ¬„ï¼ˆå°é½Šè¼¸å‡ºï¼‰
    try:
        df_ws = pd.read_csv(
            StringIO(text),
            sep=r"\s+",
            dtype=str,
            engine="python",
        )
        # è‹¥åªè®€åˆ° 1 æ¬„ï¼Œé€šå¸¸ä»£è¡¨ä¸æ˜¯å–®ç´” whitespace table
        if df_ws.shape[1] >= 2:
            return df_ws
    except Exception:
        pass

    # 3) å›ºå®šå¯¬åº¦ï¼ˆæœ€å¾Œä¿åº•ï¼‰
    return pd.read_fwf(StringIO(text), dtype=str)


def read_txt_bytes(raw: bytes, force_sep: str | None = None, force_encoding: str | None = None) -> pd.DataFrame:
    """
    TXT è®€å–ï¼ˆé‡é»ä¿®æ­£ï¼‰ï¼š
    - å…ˆè‡ªè¡Œ decodeï¼ˆerrors='replace'ï¼‰é¿å… big5/cp950 æ··ç·¨ç¢¼ç›´æ¥ç‚¸
    - å†ç”¨ StringIO ä¸Ÿçµ¦ pandas
    - è‡ªå‹• fallbackï¼šå¤šç©ºç™½ / fixed-width
    """
    encodings = [force_encoding] if force_encoding else []
    encodings += [e for e in CANDIDATE_ENCODINGS if e not in encodings]

    last_err = None
    for enc in encodings:
        try:
            text = raw.decode(enc, errors="replace")
            sep = force_sep if force_sep else _detect_sep(text)
            df = _read_txt_as_df(text, sep=sep)
            return df
        except Exception as e:
            last_err = e
            continue

    raise RuntimeError(f"TXT è®€å–å¤±æ•—ï¼ˆåˆ†éš”ç¬¦/æ ¼å¼ä¸ç¬¦ï¼‰ï¼š{last_err}")


def robust_read_file(uploaded_file, txt_sep_choice: str, txt_encoding_choice: str) -> pd.DataFrame:
    name = (uploaded_file.name or "").lower()
    raw = uploaded_file.getvalue()

    sep_map = {
        "è‡ªå‹•": None,
        "Tab": "\t",
        "é€—è™Ÿ ,": ",",
        "ç›´ç·š |": "|",
        "åˆ†è™Ÿ ;": ";",
        "å¤šç©ºç™½(å°é½Š)": None,     # äº¤çµ¦ _read_txt_as_df èµ° r'\s+' -> fwf
        "å›ºå®šå¯¬åº¦(FWF)": "__fwf__",  # ç›´æ¥èµ° read_fwf
    }
    force_sep = sep_map.get(txt_sep_choice, None)
    force_enc = None if txt_encoding_choice == "è‡ªå‹•" else txt_encoding_choice

    if name.endswith(".txt") or name.endswith(".csv"):
        return read_txt_bytes(raw, force_sep=force_sep, force_encoding=force_enc)

    bio = BytesIO(raw)
    try:
        return pd.read_excel(bio, engine="openpyxl")
    except Exception:
        try:
            bio.seek(0)
            return pd.read_excel(bio, engine="xlrd")
        except Exception as e:
            raise RuntimeError(f"è®€å– Excel å¤±æ•—ï¼š{e}")


def compute(df_raw: pd.DataFrame) -> dict:
    df_raw = _normalize_columns(df_raw)

    # æ¬„ä½å°ç…§ï¼ˆå¿½ç•¥å¤§å°å¯«/ç©ºç™½ï¼‰
    missing = [c for c in NEED_COLS if c not in df_raw.columns]
    if missing:
        lower_map = {str(c).strip().lower(): c for c in df_raw.columns}
        remap = {}
        for need in NEED_COLS:
            key = need.strip().lower()
            if key in lower_map:
                remap[lower_map[key]] = need
        if remap:
            df_raw = df_raw.rename(columns=remap)
            df_raw = _normalize_columns(df_raw)

    missing2 = [c for c in NEED_COLS if c not in df_raw.columns]
    if missing2:
        raise KeyError(
            f"âš ï¸ æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼š{missing2}\n"
            f"ç›®å‰è®€åˆ°çš„æ¬„ä½ï¼š{list(df_raw.columns)[:30]}{' ...' if len(df_raw.columns)>30 else ''}"
        )

    df0 = df_raw.copy()

    # 1) åˆªé™¤ã€Œç®±é¡å‹ã€å«ã€Œç«™æ‰€ã€
    before = len(df0)
    df = df0[~_safe_str(df0["ç®±é¡å‹"]).str.contains("ç«™æ‰€", na=False)].copy()
    removed_station = before - len(df)

    # 2) æ–°å¢æ¬„ä½
    pack = pd.to_numeric(df["packqty"], errors="coerce")
    unit = pd.to_numeric(df["å…¥æ•¸"], errors="coerce")

    df["è¨ˆé‡å–®ä½æ•¸é‡"] = np.where((unit.notna()) & (unit != 0), pack / unit, np.nan)

    v = pd.to_numeric(df["è¨ˆé‡å–®ä½æ•¸é‡"], errors="coerce")
    is_int = np.isfinite(v) & np.isclose(v, np.round(v))
    df["å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"] = np.where(is_int, v, pack)

    # æ’åœ¨ã€Œå…¥æ•¸ã€å³é‚Š
    cols = list(df.columns)
    for c in ["è¨ˆé‡å–®ä½æ•¸é‡", "å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"]:
        if c in cols:
            cols.remove(c)
    ins_pos = cols.index("å…¥æ•¸") + 1
    cols[ins_pos:ins_pos] = ["è¨ˆé‡å–®ä½æ•¸é‡", "å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"]
    df = df[cols]

    # 3) çµ±è¨ˆé®ç½©
    mask_gm = _safe_str(df["è¼‰å…·è™Ÿ"]).str.contains("GM", case=False, na=False)
    boxtype = _safe_str(df["BOXTYPE"]).str.strip()
    mask_box1 = boxtype == "1"
    mask_box0 = boxtype == "0"
    mask_not_gm = ~mask_gm

    # 4) å››é …çµ±è¨ˆ
    unique_boxid_count = (
        df.loc[mask_gm & mask_box1, "boxid"]
        .astype(str)
        .str.strip()
        .replace("", np.nan)
        .dropna()
        .nunique()
    )

    ship_unit = pd.to_numeric(df["å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"], errors="coerce")
    total_shipunit_notgm_box0 = ship_unit.loc[mask_not_gm & mask_box0].sum()
    total_shipunit_gm_box1 = ship_unit.loc[mask_gm & mask_box1].sum()
    total_shipunit_notgm_box1 = ship_unit.loc[mask_not_gm & mask_box1].sum()

    return {
        "df_processed": df,
        "removed_station": int(removed_station),
        "total_in": int(len(df_raw)),
        "total_after": int(len(df)),
        "A_gm_cases": float(unique_boxid_count),
        "B_notgm_loose_pcs": float(total_shipunit_notgm_box0),
        "C_gm_box_pcs": float(total_shipunit_gm_box1),
        "D_notgm_box_pcs": float(total_shipunit_notgm_box1),
    }


def make_excel_bytes(summary_all: pd.DataFrame, detail_all: pd.DataFrame) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        summary_all.to_excel(writer, index=False, sheet_name="çµ±è¨ˆç¸½è¡¨")
        detail_all.to_excel(writer, index=False, sheet_name="åˆä½µæ˜ç´°")
    return bio.getvalue()


# ----------------------------
# UI
# ----------------------------
card_open("ğŸ“¥ ä¸Šå‚³æ˜ç´°ï¼ˆExcel / TXTï¼Œå¯å¤šæª”ï¼‰")

colA, colB = st.columns(2)
with colA:
    txt_sep_choice = st.selectbox(
        "TXT åˆ†æ¬„æ–¹å¼",
        ["è‡ªå‹•", "Tab", "é€—è™Ÿ ,", "ç›´ç·š |", "åˆ†è™Ÿ ;", "å¤šç©ºç™½(å°é½Š)", "å›ºå®šå¯¬åº¦(FWF)"],
        index=0,
    )
with colB:
    txt_encoding_choice = st.selectbox(
        "TXT ç·¨ç¢¼",
        ["è‡ªå‹•", "utf-8-sig", "utf-8", "cp950", "big5", "latin1"],
        index=0,
    )

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³è¦è™•ç†çš„æª”æ¡ˆï¼ˆ.xlsx / .xls / .txtï¼‰",
    type=["xlsx", "xls", "xlsm", "txt", "csv"],
    accept_multiple_files=True,
)
card_close()

if not uploaded_files:
    st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆï¼ˆå¯å¤šé¸ï¼‰ã€‚")
    st.stop()

results = []
details = []
errors = []

with st.spinner("è™•ç†ä¸­â€¦"):
    for f in uploaded_files:
        fname = f.name
        try:
            df_raw = robust_read_file(f, txt_sep_choice=txt_sep_choice, txt_encoding_choice=txt_encoding_choice)
            out = compute(df_raw)

            results.append(
                {
                    "æª”å": fname,
                    "è®€å–åˆ—æ•¸": out["total_in"],
                    "åˆªé™¤ç«™æ‰€åˆ—æ•¸": out["removed_station"],
                    "è™•ç†å¾Œåˆ—æ•¸": out["total_after"],
                    "A) GMä»¶æ•¸": out["A_gm_cases"],
                    "B) ä¸€èˆ¬å€‰é›¶æ•£PCS": out["B_notgm_loose_pcs"],
                    "C) GMæˆç®±PCS": out["C_gm_box_pcs"],
                    "D) ä¸€èˆ¬å€‰æˆç®±PCS": out["D_notgm_box_pcs"],
                }
            )

            df_p = out["df_processed"].copy()
            df_p.insert(0, "ä¾†æºæª”å", fname)
            details.append(df_p)

        except Exception as e:
            errors.append({"æª”å": fname, "éŒ¯èª¤": str(e)})

if errors:
    with st.expander("âš ï¸ éƒ¨åˆ†æª”æ¡ˆè™•ç†å¤±æ•—ï¼ˆå·²ç•¥éï¼‰", expanded=True):
        st.dataframe(pd.DataFrame(errors), use_container_width=True, hide_index=True)

if not results:
    st.error("æ²’æœ‰ä»»ä½•æª”æ¡ˆæˆåŠŸè™•ç†ï¼ˆè«‹ç¢ºèªæª”æ¡ˆæ ¼å¼/è¡¨é ­/åˆ†æ¬„æ–¹å¼ï¼‰ã€‚")
    st.stop()

summary_all = pd.DataFrame(results)
detail_all = pd.concat(details, ignore_index=True) if details else pd.DataFrame()

# KPIï¼ˆå¤šæª”åˆè¨ˆï¼‰
total_files_ok = len(summary_all)
total_in = int(summary_all["è®€å–åˆ—æ•¸"].sum())
total_removed = int(summary_all["åˆªé™¤ç«™æ‰€åˆ—æ•¸"].sum())
total_after = int(summary_all["è™•ç†å¾Œåˆ—æ•¸"].sum())

A_sum = float(summary_all["A) GMä»¶æ•¸"].sum())
B_sum = float(summary_all["B) ä¸€èˆ¬å€‰é›¶æ•£PCS"].sum())
C_sum = float(summary_all["C) GMæˆç®±PCS"].sum())
D_sum = float(summary_all["D) ä¸€èˆ¬å€‰æˆç®±PCS"].sum())

st.caption(
    f"æˆåŠŸè™•ç† {total_files_ok} å€‹æª”æ¡ˆï¼›"
    f"åˆè¨ˆè®€å– {total_in:,} åˆ—ï¼›åˆªé™¤ç«™æ‰€ {total_removed:,} åˆ—ï¼›è™•ç†å¾Œ {total_after:,} åˆ—ã€‚"
)

c1, c2 = st.columns(2, gap="large")
with c1:
    render_kpis(
        [
            KPI("A) GMä»¶æ•¸ï¼ˆåˆè¨ˆï¼‰", _fmt_int(A_sum)),
            KPI("C) GMæˆç®±PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(C_sum)),
        ],
        cols=1,
    )
with c2:
    render_kpis(
        [
            KPI("B) ä¸€èˆ¬å€‰é›¶æ•£PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(B_sum)),
            KPI("D) ä¸€èˆ¬å€‰æˆç®±PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(D_sum)),
        ],
        cols=1,
    )

card_open("ğŸ“Œ å¤šæª”çµ±è¨ˆç¸½è¡¨")
show_df = summary_all.copy()
for c in ["A) GMä»¶æ•¸", "B) ä¸€èˆ¬å€‰é›¶æ•£PCS", "C) GMæˆç®±PCS", "D) ä¸€èˆ¬å€‰æˆç®±PCS"]:
    show_df[c] = show_df[c].apply(_fmt0)
st.dataframe(show_df, use_container_width=True, hide_index=True)
card_close()

card_open("ğŸ“¤ åŒ¯å‡ºï¼ˆçµ±è¨ˆç¸½è¡¨ + åˆä½µæ˜ç´°ï¼‰")
stamp = datetime.now().strftime("%Y%m%d_%H%M")
filename = f"å¤§è±KPI_æ•´é«”ä½œæ¥­é‡é«”_å¤šæª”_{stamp}.xlsx"

xlsx_bytes = make_excel_bytes(summary_all, detail_all)

download_excel_card(
    title="âœ… ä¸‹è¼‰ Excelï¼ˆå«ï¼šçµ±è¨ˆç¸½è¡¨ + åˆä½µæ˜ç´°ï¼‰",
    data=xlsx_bytes,
    filename=filename,
)

with st.expander("ğŸ” åˆä½µæ˜ç´°é è¦½ï¼ˆå‰ 200 ç­†ï¼‰", expanded=False):
    st.dataframe(detail_all.head(200), use_container_width=True)

card_close()
