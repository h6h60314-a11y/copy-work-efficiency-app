# pages/26_æ•´é«”ä½œæ¥­é‡é«”.py
# -*- coding: utf-8 -*-
from __future__ import annotations

from io import BytesIO, StringIO
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st

from common_ui import (
    inject_logistics_theme,
    set_page,
    KPI,
    render_kpis,
    card_open,
    card_close,
    download_excel_card,
)

st.set_page_config(page_title="å¤§è±KPIï½œæ•´é«”ä½œæ¥­é‡é«”", page_icon="ğŸ§¹", layout="wide")
inject_logistics_theme()

set_page(
    "æ•´é«”ä½œæ¥­é‡é«”",
    icon="ğŸ§¹",
    subtitle="æ”¯æ´ Excel/TXTï½œå¯å¤šæª”ä¸Šå‚³ï½œè‡ªå‹•åµæ¸¬TXTç·¨ç¢¼ï¼ˆè§£æ±ºä¸­æ–‡äº‚ç¢¼ï¼‰ï½œGM/ä¸€èˆ¬å€‰ Ã— æˆç®±/é›¶æ•£çµ±è¨ˆï½œExcelä¸‹è¼‰",
)

NEED_COLS = ["packqty", "å…¥æ•¸", "ç®±é¡å‹", "è¼‰å…·è™Ÿ", "BOXTYPE", "boxid"]
CANDIDATE_SEPS = ["\t", ",", "|", ";"]


def _safe_str(s: pd.Series) -> pd.Series:
    return s.astype(str).fillna("").astype(str)


def _fmt_int(x) -> str:
    try:
        return f"{int(round(float(x))):,}"
    except Exception:
        return "0"


def _fmt0(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return "0"


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df


# ----------------------------
# âœ… Encoding detection (fix garbled Chinese)
# ----------------------------
def _has_utf16_nulls(raw: bytes) -> bool:
    if len(raw) < 200:
        return raw.count(b"\x00") > 0
    head = raw[:2000]
    return head.count(b"\x00") / max(1, len(head)) > 0.08  # æœ‰æ˜é¡¯ NUL å¾ˆåƒ UTF-16


def _bom_encoding(raw: bytes) -> str | None:
    if raw.startswith(b"\xef\xbb\xbf"):
        return "utf-8-sig"
    if raw.startswith(b"\xff\xfe") or raw.startswith(b"\xfe\xff"):
        return "utf-16"
    return None


def _score_text(text: str) -> int:
    # åˆ†æ•¸è¶Šé«˜è¶Šåƒã€Œæ­£å¸¸ä¸­æ–‡è¡¨æ ¼ã€
    # 1) äº‚ç¢¼æ›¿ä»£ç¬¦è¶Šå°‘è¶Šå¥½
    repl = text.count("\ufffd")
    # 2) æ§åˆ¶å­—å…ƒè¶Šå°‘è¶Šå¥½
    ctrl = sum(1 for ch in text if ord(ch) < 32 and ch not in ("\n", "\r", "\t"))
    # 3) ä¸­æ–‡è¶Šå¤šè¶Šå¥½ï¼ˆåªåŠ åˆ†ï¼Œä¸æœƒå®³è‹±æ–‡æª”æ¡ˆè®Šå·®å¤ªå¤šï¼‰
    cjk = sum(1 for ch in text if "\u4e00" <= ch <= "\u9fff")
    # 4) ç¬¬ä¸€è¡Œè¡¨é ­å¯è®€æ€§ï¼šå¯ç”¨åˆ†éš”ç¬¦æ•¸é‡ç•¥åŠ åˆ†
    lines = [ln for ln in text.splitlines() if ln.strip()]
    first = lines[0] if lines else ""
    sep_bonus = max(first.count("\t"), first.count(","), first.count("|"), first.count(";"))

    # é€™çµ„æ¬Šé‡è¶³å¤ æŠŠã€Œä¸­æ–‡äº‚ç¢¼ã€è·Ÿã€Œä¸­æ–‡æ­£å¸¸ã€åˆ†å‡ºä¾†
    return (cjk * 2) + sep_bonus * 3 - repl * 25 - ctrl * 10


def detect_best_encoding(raw: bytes) -> tuple[str, str]:
    """
    å›å‚³ (best_encoding, preview_text_head)
    """
    bom = _bom_encoding(raw)
    if bom:
        text = raw.decode(bom, errors="replace")
        return bom, text[:4000]

    # UTF-16 ç‰¹å¾µï¼šå¤§é‡ NUL
    if _has_utf16_nulls(raw):
        for enc in ("utf-16", "utf-16le", "utf-16be"):
            try:
                text = raw.decode(enc, errors="replace")
                return enc, text[:4000]
            except Exception:
                pass

    # å€™é¸ï¼šå°ç£å¸¸è¦‹ cp950/big5 + å¯èƒ½ gb18030ï¼ˆç°¡ä¸­/æ··ç¢¼ï¼‰+ utf-8
    candidates = [
        "utf-8-sig",
        "utf-8",
        "cp950",
        "big5",
        "gb18030",
        "cp936",
        "utf-16",
        "utf-16le",
        "utf-16be",
    ]

    best_enc = "utf-8"
    best_score = -10**18
    best_text = ""
    head = raw[:400_000]  # åªæ‹¿å‰é¢ä¸€æ®µä¾†æ‰“åˆ†ï¼ˆå¤ åˆ¤æ–·è¡¨é ­/ä¸­æ–‡ï¼‰

    for enc in candidates:
        try:
            txt = head.decode(enc, errors="replace")
        except Exception:
            continue
        sc = _score_text(txt)
        if sc > best_score:
            best_score = sc
            best_enc = enc
            best_text = txt

    return best_enc, best_text[:4000]


def _detect_sep(text: str) -> str | None:
    lines = [ln for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None
    first = lines[0]
    best = None
    best_cnt = 0
    for sep in CANDIDATE_SEPS:
        cnt = first.count(sep)
        if cnt > best_cnt:
            best_cnt = cnt
            best = sep
    return best if best_cnt > 0 else None


def _read_txt_as_df(text: str, mode: str) -> pd.DataFrame:
    """
    mode:
      auto / sep:\t / sep:, / sep:| / sep:; / ws / fwf
    """
    if mode.startswith("sep:"):
        sep = mode.split(":", 1)[1]
        return pd.read_csv(StringIO(text), sep=sep, dtype=str, engine="python")

    if mode == "ws":
        return pd.read_csv(StringIO(text), sep=r"\s+", dtype=str, engine="python")

    if mode == "fwf":
        return pd.read_fwf(StringIO(text), dtype=str)

    # auto
    sep = _detect_sep(text)
    if sep is not None:
        return pd.read_csv(StringIO(text), sep=sep, dtype=str, engine="python")

    # fallback: ws -> fwf
    try:
        df_ws = pd.read_csv(StringIO(text), sep=r"\s+", dtype=str, engine="python")
        if df_ws.shape[1] >= 2:
            return df_ws
    except Exception:
        pass
    return pd.read_fwf(StringIO(text), dtype=str)


def read_txt_bytes(raw: bytes, parse_mode: str, encoding_choice: str) -> tuple[pd.DataFrame, str]:
    """
    å›å‚³ (df, used_encoding)
    - encoding_choice: "è‡ªå‹•(åµæ¸¬)" or explicit encoding string
    """
    if encoding_choice == "è‡ªå‹•(åµæ¸¬)":
        used_enc, _ = detect_best_encoding(raw)
    else:
        used_enc = encoding_choice

    text = raw.decode(used_enc, errors="replace")
    df = _read_txt_as_df(text, parse_mode)
    return df, used_enc


def robust_read_file(uploaded_file, txt_parse_choice: str, txt_encoding_choice: str) -> tuple[pd.DataFrame, str | None]:
    name = (uploaded_file.name or "").lower()
    raw = uploaded_file.getvalue()

    parse_map = {
        "è‡ªå‹•": "auto",
        "Tab": "sep:\t",
        "é€—è™Ÿ ,": "sep:,",
        "ç›´ç·š |": "sep:|",
        "åˆ†è™Ÿ ;": "sep:;",
        "å¤šç©ºç™½(å°é½Š)": "ws",
        "å›ºå®šå¯¬åº¦(FWF)": "fwf",
    }
    parse_mode = parse_map.get(txt_parse_choice, "auto")

    if name.endswith(".txt") or name.endswith(".csv"):
        df, used_enc = read_txt_bytes(raw, parse_mode=parse_mode, encoding_choice=txt_encoding_choice)
        return df, used_enc

    bio = BytesIO(raw)
    try:
        return pd.read_excel(bio, engine="openpyxl"), None
    except Exception:
        bio.seek(0)
        return pd.read_excel(bio, engine="xlrd"), None


# ----------------------------
# mapping helpers (optional)
# ----------------------------
def apply_column_mapping(df: pd.DataFrame, map_in: str | None, map_box: str | None, map_vehicle: str | None) -> pd.DataFrame:
    df = _normalize_columns(df)
    rename = {}
    if "å…¥æ•¸" not in df.columns and map_in and map_in in df.columns:
        rename[map_in] = "å…¥æ•¸"
    if "ç®±é¡å‹" not in df.columns and map_box and map_box in df.columns:
        rename[map_box] = "ç®±é¡å‹"
    if "è¼‰å…·è™Ÿ" not in df.columns and map_vehicle and map_vehicle in df.columns:
        rename[map_vehicle] = "è¼‰å…·è™Ÿ"
    return df.rename(columns=rename) if rename else df


def compute(df_raw: pd.DataFrame) -> dict:
    df_raw = _normalize_columns(df_raw)

    missing = [c for c in NEED_COLS if c not in df_raw.columns]
    if missing:
        raise KeyError(
            f"âš ï¸ æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼š{missing}\n"
            f"ç›®å‰è®€åˆ°çš„æ¬„ä½ï¼ˆå‰30ï¼‰ï¼š{list(df_raw.columns)[:30]}{' ...' if len(df_raw.columns)>30 else ''}"
        )

    df0 = df_raw.copy()

    before = len(df0)
    df = df0[~_safe_str(df0["ç®±é¡å‹"]).str.contains("ç«™æ‰€", na=False)].copy()
    removed_station = before - len(df)

    pack = pd.to_numeric(df["packqty"], errors="coerce")
    unit = pd.to_numeric(df["å…¥æ•¸"], errors="coerce")

    df["è¨ˆé‡å–®ä½æ•¸é‡"] = np.where((unit.notna()) & (unit != 0), pack / unit, np.nan)

    v = pd.to_numeric(df["è¨ˆé‡å–®ä½æ•¸é‡"], errors="coerce")
    is_int = np.isfinite(v) & np.isclose(v, np.round(v))
    df["å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"] = np.where(is_int, v, pack)

    cols = list(df.columns)
    for c in ["è¨ˆé‡å–®ä½æ•¸é‡", "å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"]:
        if c in cols:
            cols.remove(c)
    ins_pos = cols.index("å…¥æ•¸") + 1
    cols[ins_pos:ins_pos] = ["è¨ˆé‡å–®ä½æ•¸é‡", "å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"]
    df = df[cols]

    mask_gm = _safe_str(df["è¼‰å…·è™Ÿ"]).str.contains("GM", case=False, na=False)
    boxtype = _safe_str(df["BOXTYPE"]).str.strip()
    mask_box1 = boxtype == "1"
    mask_box0 = boxtype == "0"
    mask_not_gm = ~mask_gm

    unique_boxid_count = (
        df.loc[mask_gm & mask_box1, "boxid"]
        .astype(str).str.strip().replace("", np.nan).dropna().nunique()
    )

    ship_unit = pd.to_numeric(df["å‡ºè²¨å–®ä½ï¼ˆåˆ¤æ–·å¾Œï¼‰"], errors="coerce")
    total_shipunit_notgm_box0 = ship_unit.loc[mask_not_gm & mask_box0].sum()
    total_shipunit_gm_box1 = ship_unit.loc[mask_gm & mask_box1].sum()
    total_shipunit_notgm_box1 = ship_unit.loc[mask_not_gm & mask_box1].sum()

    return {
        "df_processed": df,
        "removed_station": int(removed_station),
        "total_in": int(len(df_raw)),
        "total_after": int(len(df)),
        "A_gm_cases": float(unique_boxid_count),
        "B_notgm_loose_pcs": float(total_shipunit_notgm_box0),
        "C_gm_box_pcs": float(total_shipunit_gm_box1),
        "D_notgm_box_pcs": float(total_shipunit_notgm_box1),
    }


def make_excel_bytes(summary_all: pd.DataFrame, detail_all: pd.DataFrame) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        summary_all.to_excel(writer, index=False, sheet_name="çµ±è¨ˆç¸½è¡¨")
        detail_all.to_excel(writer, index=False, sheet_name="åˆä½µæ˜ç´°")
    return bio.getvalue()


# ----------------------------
# UI
# ----------------------------
card_open("ğŸ“¥ ä¸Šå‚³æ˜ç´°ï¼ˆExcel / TXTï¼Œå¯å¤šæª”ï¼‰")
colA, colB = st.columns(2)
with colA:
    txt_parse_choice = st.selectbox(
        "TXT åˆ†æ¬„æ–¹å¼",
        ["è‡ªå‹•", "Tab", "é€—è™Ÿ ,", "ç›´ç·š |", "åˆ†è™Ÿ ;", "å¤šç©ºç™½(å°é½Š)", "å›ºå®šå¯¬åº¦(FWF)"],
        index=0,
    )
with colB:
    txt_encoding_choice = st.selectbox(
        "TXT ç·¨ç¢¼",
        ["è‡ªå‹•(åµæ¸¬)", "cp950", "big5", "utf-8-sig", "utf-8", "gb18030", "cp936", "utf-16", "utf-16le", "utf-16be"],
        index=0,
    )

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³è¦è™•ç†çš„æª”æ¡ˆï¼ˆ.xlsx / .xls / .txtï¼‰",
    type=["xlsx", "xls", "xlsm", "txt", "csv"],
    accept_multiple_files=True,
)
card_close()

if not uploaded_files:
    st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆï¼ˆå¯å¤šé¸ï¼‰ã€‚")
    st.stop()

# å…ˆç”¨ç¬¬ä¸€å€‹æª”åš previewï¼ˆé¡¯ç¤ºåµæ¸¬åˆ°çš„ç·¨ç¢¼ï¼‰
try:
    df_preview, used_enc_preview = robust_read_file(uploaded_files[0], txt_parse_choice, txt_encoding_choice)
    df_preview = _normalize_columns(df_preview)
except Exception as e:
    st.error(f"ç¬¬ä¸€å€‹æª”æ¡ˆè®€å–å¤±æ•—ï¼š{e}")
    st.stop()

if used_enc_preview:
    st.caption(f"ç¬¬ä¸€å€‹ TXT åµæ¸¬/ä½¿ç”¨çš„ç·¨ç¢¼ï¼š**{used_enc_preview}**ï¼ˆè‹¥ä¸­æ–‡ä»äº‚ç¢¼ï¼Œè«‹æ”¹ TXT ç·¨ç¢¼å†è©¦ï¼‰")

# æ¬„ä½å°ç…§ï¼ˆå¿…è¦æ™‚æ‰ç”¨ï¼‰
cols = list(df_preview.columns)
opt = ["ï¼ˆè‡ªå‹•ï¼‰"] + cols

with st.expander("ğŸ§© æ¬„ä½å°ç…§ï¼ˆè‹¥ TXT ä¸­æ–‡æ¬„åä»äº‚ç¢¼ï¼Œè«‹æ‰‹å‹•æŒ‡å®šï¼‰", expanded=False):
    col1, col2, col3 = st.columns(3)
    with col1:
        map_in = st.selectbox("å…¥æ•¸ æ¬„ä½", opt, index=0)
    with col2:
        map_box = st.selectbox("ç®±é¡å‹ æ¬„ä½", opt, index=0)
    with col3:
        map_vehicle = st.selectbox("è¼‰å…·è™Ÿ æ¬„ä½ï¼ˆç”¨ä¾†åˆ¤æ–· GMï¼‰", opt, index=0)

map_in = None if map_in == "ï¼ˆè‡ªå‹•ï¼‰" else map_in
map_box = None if map_box == "ï¼ˆè‡ªå‹•ï¼‰" else map_box
map_vehicle = None if map_vehicle == "ï¼ˆè‡ªå‹•ï¼‰" else map_vehicle

results = []
details = []
errors = []

with st.spinner("è™•ç†ä¸­â€¦"):
    for f in uploaded_files:
        fname = f.name
        try:
            df_raw, used_enc = robust_read_file(f, txt_parse_choice, txt_encoding_choice)
            df_raw = apply_column_mapping(df_raw, map_in=map_in, map_box=map_box, map_vehicle=map_vehicle)
            out = compute(df_raw)

            results.append(
                {
                    "æª”å": fname,
                    "TXTç·¨ç¢¼": used_enc or "",
                    "è®€å–åˆ—æ•¸": out["total_in"],
                    "åˆªé™¤ç«™æ‰€åˆ—æ•¸": out["removed_station"],
                    "è™•ç†å¾Œåˆ—æ•¸": out["total_after"],
                    "A) GMä»¶æ•¸": out["A_gm_cases"],
                    "B) ä¸€èˆ¬å€‰é›¶æ•£PCS": out["B_notgm_loose_pcs"],
                    "C) GMæˆç®±PCS": out["C_gm_box_pcs"],
                    "D) ä¸€èˆ¬å€‰æˆç®±PCS": out["D_notgm_box_pcs"],
                }
            )

            df_p = out["df_processed"].copy()
            df_p.insert(0, "ä¾†æºæª”å", fname)
            details.append(df_p)

        except Exception as e:
            errors.append({"æª”å": fname, "éŒ¯èª¤": str(e)})

if errors:
    with st.expander("âš ï¸ éƒ¨åˆ†æª”æ¡ˆè™•ç†å¤±æ•—ï¼ˆå·²ç•¥éï¼‰", expanded=True):
        st.dataframe(pd.DataFrame(errors), use_container_width=True, hide_index=True)

if not results:
    st.error("æ²’æœ‰ä»»ä½•æª”æ¡ˆæˆåŠŸè™•ç†ï¼šè«‹å…ˆæŠŠ TXT ç·¨ç¢¼èª¿åˆ°ä¸­æ–‡æ­£å¸¸ï¼ˆæœ€å¸¸è¦‹ cp950 / big5 / utf-16ï¼‰ã€‚")
    st.stop()

summary_all = pd.DataFrame(results)
detail_all = pd.concat(details, ignore_index=True) if details else pd.DataFrame()

# KPIï¼ˆåˆè¨ˆï¼‰
total_files_ok = len(summary_all)
total_in = int(summary_all["è®€å–åˆ—æ•¸"].sum())
total_removed = int(summary_all["åˆªé™¤ç«™æ‰€åˆ—æ•¸"].sum())
total_after = int(summary_all["è™•ç†å¾Œåˆ—æ•¸"].sum())

A_sum = float(summary_all["A) GMä»¶æ•¸"].sum())
B_sum = float(summary_all["B) ä¸€èˆ¬å€‰é›¶æ•£PCS"].sum())
C_sum = float(summary_all["C) GMæˆç®±PCS"].sum())
D_sum = float(summary_all["D) ä¸€èˆ¬å€‰æˆç®±PCS"].sum())

st.caption(
    f"æˆåŠŸè™•ç† {total_files_ok} å€‹æª”æ¡ˆï¼›"
    f"åˆè¨ˆè®€å– {total_in:,} åˆ—ï¼›åˆªé™¤ç«™æ‰€ {total_removed:,} åˆ—ï¼›è™•ç†å¾Œ {total_after:,} åˆ—ã€‚"
)

c1, c2 = st.columns(2, gap="large")
with c1:
    render_kpis(
        [
            KPI("A) GMä»¶æ•¸ï¼ˆåˆè¨ˆï¼‰", _fmt_int(A_sum)),
            KPI("C) GMæˆç®±PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(C_sum)),
        ],
        cols=1,
    )
with c2:
    render_kpis(
        [
            KPI("B) ä¸€èˆ¬å€‰é›¶æ•£PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(B_sum)),
            KPI("D) ä¸€èˆ¬å€‰æˆç®±PCSï¼ˆåˆè¨ˆï¼‰", _fmt0(D_sum)),
        ],
        cols=1,
    )

card_open("ğŸ“Œ å¤šæª”çµ±è¨ˆç¸½è¡¨")
show_df = summary_all.copy()
for c in ["A) GMä»¶æ•¸", "B) ä¸€èˆ¬å€‰é›¶æ•£PCS", "C) GMæˆç®±PCS", "D) ä¸€èˆ¬å€‰æˆç®±PCS"]:
    show_df[c] = show_df[c].apply(_fmt0)
st.dataframe(show_df, use_container_width=True, hide_index=True)
card_close()

card_open("ğŸ“¤ åŒ¯å‡ºï¼ˆçµ±è¨ˆç¸½è¡¨ + åˆä½µæ˜ç´°ï¼‰")
stamp = datetime.now().strftime("%Y%m%d_%H%M")
filename = f"å¤§è±KPI_æ•´é«”ä½œæ¥­é‡é«”_å¤šæª”_{stamp}.xlsx"
xlsx_bytes = make_excel_bytes(summary_all, detail_all)

download_excel_card(
    title="âœ… ä¸‹è¼‰ Excelï¼ˆå«ï¼šçµ±è¨ˆç¸½è¡¨ + åˆä½µæ˜ç´°ï¼‰",
    data=xlsx_bytes,
    filename=filename,
)

with st.expander("ğŸ” åˆä½µæ˜ç´°é è¦½ï¼ˆå‰ 200 ç­†ï¼‰", expanded=False):
    st.dataframe(detail_all.head(200), use_container_width=True)

card_close()
