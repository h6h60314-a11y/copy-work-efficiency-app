# pages/4_å„²ä½ä½¿ç”¨ç‡.py
# -*- coding: utf-8 -*-
"""
4_å„²ä½ä½¿ç”¨ç‡ï¼ˆéƒ¨ç½²ç‰ˆ / Streamlitï¼‰
âœ… ç‰¹è‰²ï¼š
- æ”¯æ´ xlsx / xls / xlsm / xlsb / csv
- å·¦æ¬„ï¼šå€(æº«å±¤) ä½¿ç”¨ç‡æ˜ç´°ï¼ˆå…©æ¬„æ›åˆ—ï¼šå¤§/ä¸­ï½œå°/ç¸½è¨ˆï¼‰
- å³æ¬„ï¼šæ£šåˆ¥åˆ†é¡çµ±è¨ˆï¼ˆå…©æ¬„æ›åˆ—ï¼šè¼•å‹æ–™æ¶/è½åœ°å„²ï½œé‡å‹ä½ç©º/é«˜ç©ºå„²ï¼›æœªçŸ¥å¦åˆ— + æ˜ç´°ï¼‰
- ã€Œæ£šåˆ¥çµ±è¨ˆ Top50ã€ä¸€å®šå…¨å¯¬
- æœªçŸ¥æ˜ç´°å…¨å¯¬
- æ•´é«”å­—é«”/é–“è·ç¸®å°ï¼ˆæ¯”ç…§ 18_å„é¡å„²å€ä½¿ç”¨ç‡ï¼‰
"""

import io
import os
import re
import warnings

import pandas as pd
import streamlit as st

warnings.filterwarnings("ignore")

# ---- å¥—ç”¨å¹³å°é¢¨æ ¼ï¼ˆæœ‰å°±ç”¨ï¼Œæ²’æœ‰å°±é€€å›åŸç”Ÿï¼‰----
try:
    from common_ui import inject_logistics_theme, set_page, card_open, card_close
    HAS_COMMON_UI = True
except Exception:
    HAS_COMMON_UI = False


# =========================
# âœ… å€(æº«å±¤)åˆ†å€æ¸…å–®ï¼ˆå¤§/ä¸­/å°ï¼‰
# =========================
è¼•å‹æ–™æ¶ =  ["001", "002", "003", "017", "016"],
è½åœ°å„²: ["014", "018", "019", "020", "010", "081", "401", "402", "403", "015"]
é‡å‹ä½ç©º: ["011", "012", "013", "031", "032", "033", "034", "035", "036", "037", "038"]
é«˜ç©ºå„²: [
        "021", "022", "023",
        "041", "042", "043",
        "051", "052", "053", "054", "055", "056", "057",
        "301", "302", "303", "304", "305", "306",
        "311", "312", "313", "314",
        "061"]

è¼•å‹æ–™æ¶ = set(è¼•å‹æ–™æ¶_ZONES)
è½åœ°å„² = set(è½åœ°å„²_ZONES)
é‡å‹ä½ç©º = set(é‡å‹ä½ç©º_ZONES)
é«˜ç©ºå„² = set(é«˜ç©ºå„²_ZONES)

# =========================
# âœ… æ£šåˆ¥åˆ†é¡ï¼ˆåŒæ­¥ä½ æŒ‡å®šçš„æ–°é‚è¼¯ï¼‰
# =========================
SHELF_BUCKETS = {
    "è¼•å‹æ–™æ¶": ["001", "002", "003", "017", "016"],
    "è½åœ°å„²": ["014", "018", "019", "020", "010", "081", "401", "402", "403", "015"],
    "é‡å‹ä½ç©º": ["011", "012", "013", "031", "032", "033", "034", "035", "036", "037", "038"],
    "é«˜ç©ºå„²": [
        "021", "022", "023",
        "041", "042", "043",
        "051", "052", "053", "054", "055", "056", "057",
        "301", "302", "303", "304", "305", "306",
        "311", "312", "313", "314",
        "061",
    ],
}
SHELF_BUCKET_SETS = {k: set(v) for k, v in SHELF_BUCKETS.items()}
SHELF_ORDER = ["è¼•å‹æ–™æ¶", "è½åœ°å„²", "é‡å‹ä½ç©º", "é«˜ç©ºå„²", "æœªçŸ¥"]


# =========================
# UIï¼šç¸®å°ç‰ˆï¼ˆæ¯”ç…§ 18ï¼‰
# =========================
def inject_compact_css():
    st.markdown(
        r"""
<style>
html, body, [class*="css"]{ font-size: 14px !important; }
.block-container{ padding-top: .85rem !important; padding-bottom: 1.15rem !important; }
h1{ font-size: 1.50rem !important; margin: .15rem 0 .35rem !important; }
h2{ font-size: 1.12rem !important; margin: .35rem 0 .20rem !important; }
h3{ font-size: 1.00rem !important; margin: .28rem 0 .12rem !important; }
p, li{ line-height: 1.45 !important; }
div[data-testid="stMetric"]{ padding: 6px 10px !important; }
div[data-testid="stMetric"] label{ font-size: 12px !important; }
div[data-testid="stMetric"] div{ font-size: 20px !important; }
div[data-testid="stDataFrame"]{ margin-top: .15rem !important; }
</style>
""",
        unsafe_allow_html=True,
    )


def _spacer(px: int = 10):
    st.markdown(f"<div style='height:{px}px'></div>", unsafe_allow_html=True)


# =========================
# è®€æª”ï¼šæ”¯æ´ xlsb
# =========================
def detect_sheet_for_column(xls: pd.ExcelFile, must_have: str, engine: str | None = None) -> str:
    for name in xls.sheet_names:
        try:
            df0 = pd.read_excel(xls, sheet_name=name, nrows=0, engine=engine)
            if must_have in df0.columns:
                return name
        except Exception:
            continue
    return xls.sheet_names[0]


def robust_read_uploaded(uploaded) -> tuple[pd.DataFrame, str]:
    filename = uploaded.name
    ext = os.path.splitext(filename)[1].lower()
    data = uploaded.getvalue()
    bio = io.BytesIO(data)

    # CSV
    if ext == ".csv":
        df = pd.read_csv(bio, encoding="utf-8-sig")
        return df, "CSV"

    # XLSB
    if ext == ".xlsb":
        xls = pd.ExcelFile(bio, engine="pyxlsb")
        sheet = None
        for key in ["å€(æº«å±¤)", "æ£šåˆ¥"]:
            candidate = detect_sheet_for_column(xls, key, engine="pyxlsb")
            try:
                cols = pd.read_excel(xls, sheet_name=candidate, nrows=0, engine="pyxlsb").columns
                if key in cols:
                    sheet = candidate
                    break
            except Exception:
                pass
        if sheet is None:
            sheet = xls.sheet_names[0]
        df = pd.read_excel(xls, sheet_name=sheet, engine="pyxlsb")
        return df, sheet

    # XLS / XLSX
    if ext in (".xlsx", ".xlsm", ".xltx", ".xltm"):
        engine = "openpyxl"
    elif ext == ".xls":
        engine = "xlrd"
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{ext}")

    xls = pd.ExcelFile(bio, engine=engine)

    sheet = None
    for key in ["å€(æº«å±¤)", "æ£šåˆ¥"]:
        candidate = detect_sheet_for_column(xls, key, engine=None)
        try:
            cols = pd.read_excel(xls, sheet_name=candidate, nrows=0).columns
            if key in cols:
                sheet = candidate
                break
        except Exception:
            pass

    if sheet is None:
        sheet = xls.sheet_names[0]

    df = pd.read_excel(xls, sheet_name=sheet)
    return df, sheet


# =========================
# è¨ˆç®—ï¼šå€(æº«å±¤)ä½¿ç”¨ç‡
# =========================
def _safe_sum(s: pd.Series) -> float:
    return pd.to_numeric(s, errors="coerce").fillna(0).sum()


def calc_util_by_zone(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()
    df2["å€(æº«å±¤)"] = (
        df2["å€(æº«å±¤)"].astype(str).str.strip().replace({"nan": "", "None": "", "": ""}).str.zfill(3)
    )

    if "æœ‰æ•ˆè²¨ä½" not in df2.columns:
        df2["æœ‰æ•ˆè²¨ä½"] = 0
    if "å·²ä½¿ç”¨è²¨ä½" not in df2.columns:
        df2["å·²ä½¿ç”¨è²¨ä½"] = 0

    def _row(kind: str, zones: set) -> dict:
        part = df2[df2["å€(æº«å±¤)"].isin(zones)]
        eff = float(_safe_sum(part["æœ‰æ•ˆè²¨ä½"]))
        used = float(_safe_sum(part["å·²ä½¿ç”¨è²¨ä½"]))
        unused = max(eff - used, 0.0)
        rate = (used / eff * 100.0) if eff else 0.0
        return {
            "å„²å€": kind,
            "æœ‰æ•ˆè²¨ä½": int(round(eff)),
            "å·²ä½¿ç”¨è²¨ä½": int(round(used)),
            "æœªä½¿ç”¨è²¨ä½": int(round(unused)),
            "ä½¿ç”¨ç‡(%)": round(rate, 2),
        }

    rows = [
        _row("å¤§å„²ä½", LARGE),
        _row("ä¸­å„²ä½", MID),
        _row("å°å„²ä½", SMALL),
    ]

    eff_total = sum(r["æœ‰æ•ˆè²¨ä½"] for r in rows)
    used_total = sum(r["å·²ä½¿ç”¨è²¨ä½"] for r in rows)
    unused_total = max(eff_total - used_total, 0)
    rate_total = (used_total / eff_total * 100.0) if eff_total else 0.0

    rows.append(
        {
            "å„²å€": "ç¸½è¨ˆ",
            "æœ‰æ•ˆè²¨ä½": int(eff_total),
            "å·²ä½¿ç”¨è²¨ä½": int(used_total),
            "æœªä½¿ç”¨è²¨ä½": int(unused_total),
            "ä½¿ç”¨ç‡(%)": round(rate_total, 2),
        }
    )
    return pd.DataFrame(rows)


def render_util_block(title: str, r: dict):
    st.markdown(f"### {title}")
    st.markdown(f"**æœ‰æ•ˆè²¨ä½ï¼š** {int(r.get('æœ‰æ•ˆè²¨ä½', 0)):,}")
    st.markdown(f"**å·²ä½¿ç”¨è²¨ä½ï¼š** {int(r.get('å·²ä½¿ç”¨è²¨ä½', 0)):,}")
    st.markdown(f"**æœªä½¿ç”¨è²¨ä½ï¼š** {int(r.get('æœªä½¿ç”¨è²¨ä½', 0)):,}")
    st.markdown(f"**ä½¿ç”¨ç‡(%)ï¼š** {float(r.get('ä½¿ç”¨ç‡(%)', 0)):.2f}")


# =========================
# è¨ˆç®—ï¼šæ£šåˆ¥åˆ†é¡
# =========================
def _to_zone3(x) -> str:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""
    s = str(x).strip()
    m = re.search(r"\d{3}", s)
    if m:
        return m.group(0)
    s = re.sub(r"\D", "", s)
    return s.zfill(3) if s else ""


def classify_from_shelf_bucket(x) -> str:
    z = _to_zone3(x)
    if not z:
        return "æœªçŸ¥"
    for bucket, zset in SHELF_BUCKET_SETS.items():
        if z in zset:
            return bucket
    return "æœªçŸ¥"


# =========================
# åŒ¯å‡º Excel
# =========================
def build_output_excel_bytes(
    base_name: str,
    df_util: pd.DataFrame,
    df_detail: pd.DataFrame,
    df_shelf: pd.DataFrame,
    df_type: pd.DataFrame,
    df_unknown: pd.DataFrame,
) -> tuple[str, bytes]:
    out = io.BytesIO()
    with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
        df_util.to_excel(writer, sheet_name="å€(æº«å±¤)ä½¿ç”¨ç‡", index=False)
        df_detail.to_excel(writer, sheet_name="æ˜ç´°(å«æ£šåˆ¥åˆ†é¡)", index=False)
        df_shelf.to_excel(writer, sheet_name="æ£šåˆ¥çµ±è¨ˆ", index=False)
        df_type.to_excel(writer, sheet_name="æ£šåˆ¥åˆ†é¡çµ±è¨ˆ", index=False)
        df_unknown.to_excel(writer, sheet_name="æœªçŸ¥æ˜ç´°", index=False)
    out.seek(0)
    return f"{base_name}_4_å„²ä½ä½¿ç”¨ç‡_è¼¸å‡º.xlsx", out.getvalue()


# =========================
# Main
# =========================
def main():
    st.set_page_config(page_title="å„²ä½ä½¿ç”¨ç‡", page_icon="ğŸ§Š", layout="wide")

    if HAS_COMMON_UI:
        inject_logistics_theme()
        set_page("å„²ä½ä½¿ç”¨ç‡", icon="ğŸ§Š", subtitle="å€(æº«å±¤)ä½¿ç”¨ç‡ + æ£šåˆ¥åˆ†é¡ï¼ˆæ”¯æ´ xlsbï¼‰")
    else:
        st.title("ğŸ§Š å„²ä½ä½¿ç”¨ç‡")

    inject_compact_css()

    if HAS_COMMON_UI:
        card_open("ğŸ“¤ ä¸Šå‚³ Excelï¼ˆå„²ä½æ˜ç´°ï¼‰")
    uploaded = st.file_uploader(
        "è«‹ä¸Šå‚³æª”æ¡ˆï¼ˆxlsx/xls/xlsm/xlsb/csvï¼‰",
        type=["xlsx", "xls", "xlsm", "xlsb", "csv"],
        label_visibility="collapsed",
    )
    if HAS_COMMON_UI:
        card_close()

    if not uploaded:
        st.info("è«‹å…ˆä¸Šå‚³å„²ä½æ˜ç´°æª”æ¡ˆã€‚")
        return

    # è®€æª”
    try:
        df, sheet_used = robust_read_uploaded(uploaded)
    except Exception as e:
        st.error(f"è®€å–å¤±æ•—ï¼š{e}")
        return

    df.columns = df.columns.astype(str).str.strip()
    st.caption(f"ä½¿ç”¨åˆ†é ï¼š{sheet_used}")

    # å…©æ¬„
    left, right = st.columns(2, gap="large")

    # -------------------------
    # å·¦æ¬„ï¼šå€(æº«å±¤)åˆ†é¡
    # -------------------------
    with left:
        if HAS_COMMON_UI:
            card_open("ğŸ“Œ å€(æº«å±¤)åˆ†é¡ï¼ˆKPI + å¡ç‰‡ + åœ–è¡¨ï¼‰")

        need_cols = ["å€(æº«å±¤)", "æœ‰æ•ˆè²¨ä½", "å·²ä½¿ç”¨è²¨ä½"]
        missing = [c for c in need_cols if c not in df.columns]

        if missing:
            st.warning("âš ï¸ æ­¤æª”æ¡ˆç¼ºå°‘ã€å€(æº«å±¤)åˆ†é¡ã€å¿…è¦æ¬„ä½ï¼Œå·²è·³éæ­¤æ®µã€‚")
            st.write("ç¼ºå°‘æ¬„ä½ï¼š")
            st.code(missing, language="python")
            df_util = pd.DataFrame()
        else:
            df_util = calc_util_by_zone(df)
            util_rows = {r["å„²å€"]: r for _, r in df_util.iterrows()}

            r1c1, r1c2 = st.columns(2, gap="large")
            with r1c1:
                render_util_block("å¤§å„²ä½", util_rows.get("å¤§å„²ä½", {}))
            with r1c2:
                render_util_block("ä¸­å„²ä½", util_rows.get("ä¸­å„²ä½", {}))

            _spacer(6)

            r2c1, r2c2 = st.columns(2, gap="large")
            with r2c1:
                render_util_block("å°å„²ä½", util_rows.get("å°å„²ä½", {}))
            with r2c2:
                render_util_block("ç¸½è¨ˆ", util_rows.get("ç¸½è¨ˆ", {}))

        if HAS_COMMON_UI:
            card_close()

    # -------------------------
    # å³æ¬„ï¼šæ£šåˆ¥åˆ†é¡çµ±è¨ˆï¼ˆåŒæ­¥æ–°é‚è¼¯ï¼‰
    # -------------------------
    with right:
        if HAS_COMMON_UI:
            card_open("ğŸ·ï¸ æ£šåˆ¥åˆ†é¡çµ±è¨ˆï¼ˆè¼•å‹æ–™æ¶/è½åœ°å„²/é‡å‹ä½ç©º/é«˜ç©ºå„²/æœªçŸ¥ï¼‰")

        if "æ£šåˆ¥" not in df.columns:
            st.error("âŒ æ‰¾ä¸åˆ°æ¬„ä½ã€æ£šåˆ¥ã€ï¼Œç„¡æ³•é€²è¡Œæ£šåˆ¥åˆ†é¡ã€‚")
            df_detail = df.copy()
            df_detail["å„²ä½é¡å‹"] = "æœªçŸ¥"
            df_shelf = pd.DataFrame()
            df_type = pd.DataFrame()
            df_unknown = df_detail.copy()
            type_map = {k: 0 for k in SHELF_ORDER}
            type_map["æœªçŸ¥"] = len(df_unknown)
        else:
            df_detail = df.copy()
            df_detail["å„²ä½é¡å‹"] = df_detail["æ£šåˆ¥"].apply(classify_from_shelf_bucket)

            df_shelf = (
                df_detail.groupby(["æ£šåˆ¥"], dropna=False)
                .size()
                .reset_index(name="ç­†æ•¸")
                .sort_values(["ç­†æ•¸", "æ£šåˆ¥"], ascending=[False, True])
            )

            df_type = (
                df_detail.groupby(["å„²ä½é¡å‹"], dropna=False)
                .size()
                .reset_index(name="ç­†æ•¸")
            )

            # å›ºå®šæ’åº
            df_type["__ord"] = df_type["å„²ä½é¡å‹"].apply(
                lambda x: SHELF_ORDER.index(x) if x in SHELF_ORDER else 999
            )
            df_type = df_type.sort_values(["__ord", "å„²ä½é¡å‹"]).drop(columns="__ord")

            type_map = {k: 0 for k in SHELF_ORDER}
            for _, r in df_type.iterrows():
                type_map[str(r["å„²ä½é¡å‹"])] = int(r["ç­†æ•¸"])

            df_unknown = df_detail[df_detail["å„²ä½é¡å‹"] == "æœªçŸ¥"].copy()

            # âœ… å…©æ¬„æ›åˆ—ï¼ˆè¼•å‹/è½åœ°ï½œé‡å‹/é«˜ç©ºï¼‰
            r1c1, r1c2 = st.columns(2, gap="large")
            with r1c1:
                st.markdown("### è¼•å‹æ–™æ¶")
                st.markdown(f"**{type_map.get('è¼•å‹æ–™æ¶', 0):,} ç­†**")
            with r1c2:
                st.markdown("### è½åœ°å„²")
                st.markdown(f"**{type_map.get('è½åœ°å„²', 0):,} ç­†**")

            _spacer(6)

            r2c1, r2c2 = st.columns(2, gap="large")
            with r2c1:
                st.markdown("### é‡å‹ä½ç©º")
                st.markdown(f"**{type_map.get('é‡å‹ä½ç©º', 0):,} ç­†**")
            with r2c2:
                st.markdown("### é«˜ç©ºå„²")
                st.markdown(f"**{type_map.get('é«˜ç©ºå„²', 0):,} ç­†**")

            _spacer(6)

            # æœªçŸ¥ï¼ˆå–®ç¨åˆ—ï¼‰
            st.markdown("### æœªçŸ¥")
            st.markdown(f"**{type_map.get('æœªçŸ¥', 0):,} ç­†**")

        # åŒ¯å‡ºæŒ‰éˆ•ï¼ˆåœ¨å³æ¬„åº•éƒ¨ï¼‰
        base = os.path.splitext(uploaded.name)[0]
        df_util_export = (
            df_util
            if isinstance(df_util, pd.DataFrame) and (not df_util.empty)
            else pd.DataFrame(
                [
                    {
                        "å„²å€": "ï¼ˆç¼ºå°‘å€(æº«å±¤)æ¬„ä½ï¼‰",
                        "æœ‰æ•ˆè²¨ä½": 0,
                        "å·²ä½¿ç”¨è²¨ä½": 0,
                        "æœªä½¿ç”¨è²¨ä½": 0,
                        "ä½¿ç”¨ç‡(%)": 0.0,
                    }
                ]
            )
        )

        out_name, out_bytes = build_output_excel_bytes(
            base_name=base,
            df_util=df_util_export,
            df_detail=df_detail,
            df_shelf=df_shelf if isinstance(df_shelf, pd.DataFrame) else pd.DataFrame(),
            df_type=df_type if isinstance(df_type, pd.DataFrame) else pd.DataFrame(),
            df_unknown=df_unknown if isinstance(df_unknown, pd.DataFrame) else pd.DataFrame(),
        )

        _spacer(10)
        st.download_button(
            "â¬‡ï¸ åŒ¯å‡ºï¼ˆæ£šåˆ¥åˆ†é¡çµ±è¨ˆ Excelï¼‰",
            data=out_bytes,
            file_name=out_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )

        if HAS_COMMON_UI:
            card_close()

    # =========================
    # âœ… ä¸‹æ–¹ï¼šå…¨å¯¬ï¼ˆTop50 ä¸€å®šè¦å…¨å¯¬ï¼‰
    # =========================
    if "æ£šåˆ¥" in df.columns and isinstance(df_shelf, pd.DataFrame) and (not df_shelf.empty):
        _spacer(10)
        if HAS_COMMON_UI:
            card_open("ğŸ“‹ æ£šåˆ¥çµ±è¨ˆï¼ˆTop 50ï¼‰")
        st.dataframe(df_shelf.head(50), use_container_width=True, hide_index=True)
        if HAS_COMMON_UI:
            card_close()

    # =========================
    # âœ… ä¸‹æ–¹ï¼šæœªçŸ¥æ˜ç´°ï¼ˆå…¨å¯¬ï¼‰
    # =========================
    if "æ£šåˆ¥" in df.columns and isinstance(df_unknown, pd.DataFrame):
        unknown_cnt = int(type_map.get("æœªçŸ¥", 0)) if isinstance(type_map, dict) else 0
        _spacer(8)
        with st.expander(f"ğŸ“Œ æœªçŸ¥æ˜ç´°ï¼ˆ{unknown_cnt:,} ç­†ï¼‰", expanded=False):
            if unknown_cnt == 0:
                st.success("æœªçŸ¥ï¼š0 ç­†")
            else:
                st.dataframe(df_unknown, use_container_width=True, hide_index=True)


if __name__ == "__main__":
    main()
