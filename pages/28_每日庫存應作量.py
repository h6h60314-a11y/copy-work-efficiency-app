# -*- coding: utf-8 -*-
# pages/28_æ¯æ—¥åº«å­˜æ‡‰ä½œé‡.py

import io
import inspect
import pandas as pd
import streamlit as st

# ---- å¥—ç”¨å¹³å°é¢¨æ ¼ï¼ˆæœ‰å°±ç”¨ï¼Œæ²’æœ‰å°±é€€å›åŸç”Ÿï¼‰----
try:
    from common_ui import (
        inject_logistics_theme,
        set_page,
        card_open,
        card_close,
        download_excel_card,
    )
    HAS_COMMON_UI = True
except Exception:
    HAS_COMMON_UI = False


# =============================
# helpers
# =============================
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    new_cols = []
    for c in df.columns:
        s = str(c).replace("\u3000", " ").strip()  # å…¨å½¢ç©ºç™½ -> åŠå½¢ + strip
        new_cols.append(s)
    df.columns = new_cols
    return df


def ensure_order_sku_column(df_order: pd.DataFrame) -> pd.DataFrame:
    """è¨‚å–®æª”ï¼šå¼·åˆ¶å°é½Šã€å•†å“ã€æ¬„ä½ï¼ˆå«ï¼šå•†å“ å°¾å·´ç©ºç™½ï¼‰"""
    df_order = normalize_columns(df_order)
    if "å•†å“" in df_order.columns:
        return df_order

    candidates = ["å•†å“ç¢¼", "å•†å“ä»£è™Ÿ", "å•†å“è™Ÿ", "å“è™Ÿ", "ITEM", "SKU", "SKU#", "Item", "item"]
    for c in candidates:
        if c in df_order.columns:
            return df_order.rename(columns={c: "å•†å“"})

    raise ValueError(f"è¨‚å–®æª”ç¼ºå°‘æ¬„ä½ã€å•†å“ã€ã€‚ç›®å‰æ¬„ä½ï¼š{list(df_order.columns)}")


def format_code(x, length: int) -> str:
    if pd.isna(x) or str(x).strip() == "":
        return ""
    s = str(x).strip().split(".")[0].strip()
    return s.zfill(length)


def _is_fake_xls(raw: bytes) -> bool:
    head = raw[:2048].upper()
    return (b"<HTML" in head) or (b"<TABLE" in head) or (b"PROVIDER" in head)


def _read_fake_xls_text_or_html(raw: bytes) -> pd.DataFrame:
    for enc in ("utf-8-sig", "utf-8", "cp950", "big5"):
        try:
            text = raw.decode(enc, errors="replace")
            break
        except Exception:
            text = None
    if text is None:
        text = raw.decode("utf-8", errors="replace")

    if "<table" in text.lower():
        dfs = pd.read_html(io.StringIO(text))
        if not dfs:
            raise ValueError("åµæ¸¬ç‚º HTMLï¼Œä½†æ‰¾ä¸åˆ° table")
        return dfs[0]

    # tab -> comma
    try:
        df = pd.read_csv(io.StringIO(text), sep="\t")
        if df.shape[1] >= 2:
            return df
    except Exception:
        pass

    return pd.read_csv(io.StringIO(text), sep=",")


def robust_read_table(uploaded) -> pd.DataFrame:
    name = (uploaded.name or "").lower()
    raw = uploaded.getvalue()

    if name.endswith(".csv"):
        try:
            return pd.read_csv(io.BytesIO(raw), encoding="utf-8-sig")
        except Exception:
            return pd.read_csv(io.BytesIO(raw), encoding="big5", errors="replace")

    if name.endswith((".xlsx", ".xlsm", ".xltx", ".xltm")):
        return pd.read_excel(io.BytesIO(raw), engine="openpyxl")

    if name.endswith(".xls"):
        if _is_fake_xls(raw):
            return _read_fake_xls_text_or_html(raw)
        try:
            import xlrd  # noqa: F401
        except Exception:
            raise ValueError(
                "ä½ ä¸Šå‚³çš„æ˜¯ã€çœŸ .xlsï¼ˆèˆŠç‰ˆ Excelï¼‰ã€ï¼Œéƒ¨ç½²ç’°å¢ƒéœ€å®‰è£ xlrd æ‰èƒ½è®€ã€‚\n"
                "è«‹åœ¨ requirements.txt åŠ ä¸Šï¼šxlrd>=2.0.1\n"
                "æˆ–å…ˆæŠŠæª”æ¡ˆå¦å­˜æˆ .xlsx / .csv å†ä¸Šå‚³ã€‚"
            )
        return pd.read_excel(io.BytesIO(raw), engine="xlrd")

    return pd.read_excel(io.BytesIO(raw))


def read_master_file(uploaded) -> tuple[pd.DataFrame, pd.DataFrame]:
    raw = uploaded.getvalue()
    name = (uploaded.name or "").lower()

    if name.endswith(".xls"):
        if _is_fake_xls(raw):
            raise ValueError("å•†å“ä¸»æª”ä¸æ‡‰æ˜¯ã€å‡ xlsã€æ ¼å¼ï¼Œè«‹æä¾›æ­£å¸¸ Excelï¼ˆå«åˆ†é ï¼‰ã€‚")
        try:
            import xlrd  # noqa: F401
        except Exception:
            raise ValueError(
                "å•†å“ä¸»æª”æ˜¯ .xlsï¼Œéƒ¨ç½²ç’°å¢ƒéœ€å®‰è£ xlrdã€‚\n"
                "è«‹åœ¨ requirements.txt åŠ ä¸Šï¼šxlrd>=2.0.1\n"
                "æˆ–å…ˆå¦å­˜æˆ .xlsx å†ä¸Šå‚³ã€‚"
            )
        engine = "xlrd"
    else:
        engine = "openpyxl"

    try:
        df_master = pd.read_excel(io.BytesIO(raw), sheet_name="å•†å“ä¸»æª”", engine=engine)
        df_weight = pd.read_excel(io.BytesIO(raw), sheet_name="å¤§é¡åŠ æ¬Š", engine=engine)
        return normalize_columns(df_master), normalize_columns(df_weight)
    except Exception as e:
        raise ValueError("æ‰¾ä¸åˆ°ã€å•†å“ä¸»æª”ã€æˆ–ã€å¤§é¡åŠ æ¬Šã€åˆ†é ï¼Œè«‹æª¢æŸ¥ Excel å·¥ä½œè¡¨åç¨±ã€‚") from e


def safe_download_card(label: str, data: bytes, filename: str, mime: str = "text/csv"):
    """ç›¸å®¹ä¸åŒç‰ˆæœ¬ download_excel_card + æœ€çµ‚ä¿åº• st.download_button"""
    if HAS_COMMON_UI and "download_excel_card" in globals():
        fn = download_excel_card
        try:
            sig = inspect.signature(fn)
            params = set(sig.parameters.keys())
            kwargs = {}

            for k in ("title", "label", "text"):
                if k in params:
                    kwargs[k] = label
                    break
            for k in ("data", "xlsx_bytes", "bytes_data"):
                if k in params:
                    kwargs[k] = data
                    break
            for k in ("filename", "file_name"):
                if k in params:
                    kwargs[k] = filename
                    break
            if "mime" in params:
                kwargs["mime"] = mime

            if kwargs:
                return fn(**kwargs)
        except Exception:
            pass

        for args in [(label, data, filename), (data, filename), (label, data)]:
            try:
                return fn(*args)
            except Exception:
                continue

    return st.download_button(label, data=data, file_name=filename, mime=mime, use_container_width=True)


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def concat_orders(order_files) -> tuple[pd.DataFrame, list[str]]:
    """å¤šæª”è¨‚å–®åˆä½µï¼ˆå« ä¾†æºæª”æ¡ˆ è¿½æº¯ï¼‰"""
    msgs = []
    dfs = []
    for f in order_files:
        df = robust_read_table(f)
        df = normalize_columns(df)
        df = ensure_order_sku_column(df)
        df["ä¾†æºæª”æ¡ˆ"] = f.name
        dfs.append(df)
        msgs.append(f"å·²è®€å–ï¼š{f.name}ï¼ˆ{len(df):,} ç­†ï¼‰")

    if not dfs:
        raise ValueError("å°šæœªé¸æ“‡ä»»ä½•è¨‚å–®æª”")

    out = pd.concat(dfs, ignore_index=True, sort=False)
    msgs.append(f"å¤šæª”åˆä½µå®Œæˆï¼š{len(dfs)} æª”ï¼Œå…± {len(out):,} ç­†")
    return out, msgs


def build_result(df_order: pd.DataFrame, df_master: pd.DataFrame, df_weight: pd.DataFrame):
    msgs: list[str] = []

    df_order = ensure_order_sku_column(df_order)

    # æ’é™¤ç‰¹æ®Šå„²ä½
    exclude_list = ["CGS", "JCPL", "QC99", "PD99", "GX010", "GREAT0001X"]
    if "å„²ä½" in df_order.columns:
        before = len(df_order)
        df_order = df_order.copy()
        df_order["å„²ä½"] = df_order["å„²ä½"].astype(str).str.strip()
        pattern = "|".join(exclude_list)
        df_order = df_order[~df_order["å„²ä½"].str.contains(pattern, case=False, na=False)]
        after = len(df_order)
        msgs.append(f"å·²æ’é™¤ç‰¹æ®Šå„²ä½ï¼š{before - after:,} ç­†ï¼ˆå‰©é¤˜ {after:,} ç­†ï¼‰")

    # å¿…è¦æ¬„ä½æª¢æŸ¥
    for col in ("å•†å“ä»£è™Ÿ", "å¤§é¡"):
        if col not in df_master.columns:
            raise ValueError(f"å•†å“ä¸»æª”ç¼ºå°‘æ¬„ä½ã€{col}ã€")
    for col in ("PA", "PARM_VALUE2"):
        if col not in df_weight.columns:
            raise ValueError(f"å¤§é¡åŠ æ¬Šåˆ†é ç¼ºå°‘æ¬„ä½ã€{col}ã€")

    # è£œç¢¼
    df_order = df_order.copy()
    df_master = df_master.copy()
    df_weight = df_weight.copy()

    df_order["å•†å“"] = df_order["å•†å“"].apply(lambda x: format_code(x, 6))
    df_master["å•†å“ä»£è™Ÿ"] = df_master["å•†å“ä»£è™Ÿ"].apply(lambda x: format_code(x, 6))

    df_master["å¤§é¡"] = df_master["å¤§é¡"].apply(lambda x: format_code(x, 2))
    df_weight["PA"] = df_weight["PA"].apply(lambda x: format_code(x, 2))

    # Join Aï¼šå•†å“ -> å¤§é¡
    master_cols = ["å•†å“ä»£è™Ÿ", "å¤§é¡"]
    if "é¡åˆ¥" in df_master.columns:
        master_cols.append("é¡åˆ¥")
    df_master_sub = df_master[master_cols].drop_duplicates(subset=["å•†å“ä»£è™Ÿ"])
    step1_df = pd.merge(df_order, df_master_sub, left_on="å•†å“", right_on="å•†å“ä»£è™Ÿ", how="left")

    miss_master_mask = step1_df["å¤§é¡"].isna() | (step1_df["å¤§é¡"].astype(str).str.strip() == "")
    miss_master_df = step1_df.loc[miss_master_mask].copy()

    # Join Bï¼šå¤§é¡ -> åŠ æ¬Šå€¼
    df_weight_sub = df_weight[["PA", "PARM_VALUE2"]].drop_duplicates(subset=["PA"])
    final_df = pd.merge(step1_df, df_weight_sub, left_on="å¤§é¡", right_on="PA", how="left")
    final_df = final_df.rename(columns={"PARM_VALUE2": "å¤§é¡åŠ æ¬Šå€¼"})

    weight_missing_mask = (
        (~final_df["å¤§é¡"].isna())
        & (final_df["å¤§é¡"].astype(str).str.strip() != "")
        & (final_df["å¤§é¡åŠ æ¬Šå€¼"].isna() | (final_df["å¤§é¡åŠ æ¬Šå€¼"].astype(str).str.strip() == ""))
    )
    miss_weight_df = final_df.loc[weight_missing_mask].copy()

    # è¨ˆç®—åŠ æ¬Šçµæœ
    qty_col = "è¨ˆé‡å–®ä½æ•¸é‡"
    weight_col = "å¤§é¡åŠ æ¬Šå€¼"
    if qty_col in final_df.columns and weight_col in final_df.columns:
        final_df[qty_col] = pd.to_numeric(final_df[qty_col], errors="coerce").fillna(0)
        final_df[weight_col] = pd.to_numeric(final_df[weight_col], errors="coerce").fillna(0)
        final_df["åŠ æ¬Šè¨ˆç®—çµæœ"] = final_df[weight_col] * final_df[qty_col]
        msgs.append("å·²å®Œæˆè¨ˆç®—ï¼šåŠ æ¬Šè¨ˆç®—çµæœ = å¤§é¡åŠ æ¬Šå€¼ * è¨ˆé‡å–®ä½æ•¸é‡")

    final_df = final_df.drop(columns=["å•†å“ä»£è™Ÿ", "PA"], errors="ignore")
    miss_master_df = miss_master_df.drop(columns=["å•†å“ä»£è™Ÿ"], errors="ignore")
    miss_weight_df = miss_weight_df.drop(columns=["å•†å“ä»£è™Ÿ", "PA"], errors="ignore")

    msgs.append(f"æœªå°æ‡‰å•†å“ä¸»æª”ï¼ˆå¤§é¡ç©ºç™½ï¼‰ï¼š{len(miss_master_df):,} ç­†")
    msgs.append(f"æœªå°æ‡‰å¤§é¡åŠ æ¬Šï¼ˆå¤§é¡åŠ æ¬Šå€¼ç¼ºå¤±ï¼‰ï¼š{len(miss_weight_df):,} ç­†")

    return final_df, msgs, miss_master_df, miss_weight_df


# =============================
# UI
# =============================
def main():
    st.set_page_config(page_title="æ¯æ—¥åº«å­˜æ‡‰ä½œé‡", page_icon="ğŸ“¦", layout="wide")

    # âœ… ä½ æŒ‡å®šï¼šä¸Šæ–¹åªä¿ç•™ã€ŒğŸ“¦ æ¯æ—¥åº«å­˜æ‡‰ä½œé‡ã€
    if HAS_COMMON_UI:
        inject_logistics_theme()
        # ä¸è¦ subtitleï¼ˆé¿å…å‡ºç¾é‚£ä¸²å°å¼•æ–‡å­—ï¼‰
        try:
            set_page("æ¯æ—¥åº«å­˜æ‡‰ä½œé‡", icon="ğŸ“¦")  # å¤šæ•¸ common_ui ç‰ˆæœ¬æ”¯æ´
        except TypeError:
            # è‹¥ä½ çš„ set_page æ²’æœ‰ icon åƒæ•¸ï¼Œå°±é€€å›æœ€ç°¡å–®
            set_page("ğŸ“¦ æ¯æ—¥åº«å­˜æ‡‰ä½œé‡")
    else:
        st.title("ğŸ“¦ æ¯æ—¥åº«å­˜æ‡‰ä½œé‡")

    # ä¸Šå‚³å€
    if HAS_COMMON_UI:
        card_open("1) ä¸Šå‚³æª”æ¡ˆ")

    c1, c2 = st.columns(2)
    with c1:
        order_files = st.file_uploader(
            "è¨‚å–®è³‡æ–™æª”ï¼ˆæŠ“ã€å•†å“ã€ï½œå¯é¸æ“‡å¤šæª”ï¼‰",
            type=["csv", "xlsx", "xls", "xlsm"],
            accept_multiple_files=True,
            key="order_multi",
        )
    with c2:
        master_file = st.file_uploader(
            "å•†å“ä¸»æª”ï¼ˆå«ï¼šå•†å“ä¸»æª” / å¤§é¡åŠ æ¬Šï¼‰",
            type=["xlsx", "xls", "xlsm"],
            key="master",
        )

    if HAS_COMMON_UI:
        card_close()

    st.divider()

    run = st.button("âœ… é–‹å§‹è™•ç†", type="primary", disabled=not (order_files and master_file))
    if not run:
        return

    try:
        with st.spinner("è®€å–è¨‚å–®æª”ï¼ˆå¤šæª”ï¼‰..."):
            df_order, read_msgs = concat_orders(order_files)

        with st.spinner("è®€å–å•†å“ä¸»æª”..."):
            df_master, df_weight = read_master_file(master_file)

        with st.spinner("è™•ç†ä¸­ï¼ˆæ’é™¤ / è£œç¢¼ / Join / è¨ˆç®—ï¼‰..."):
            final_df, msgs, miss_master_df, miss_weight_df = build_result(df_order, df_master, df_weight)

        # âœ… KPIï¼šç›´å‘é¡¯ç¤ºï¼ˆä¸Šä¸‹å †ç–Šï¼‰
        total_rows = len(final_df)
        uniq_sku = final_df["å•†å“"].nunique() if "å•†å“" in final_df.columns else 0
        sum_weighted = float(final_df["åŠ æ¬Šè¨ˆç®—çµæœ"].sum()) if "åŠ æ¬Šè¨ˆç®—çµæœ" in final_df.columns else 0.0

        st.metric("ç­†æ•¸", f"{total_rows:,}")
        st.metric("å•†å“æ•¸(ä¸é‡è¤‡)", f"{uniq_sku:,}")
        st.metric("åŠ æ¬Šè¨ˆç®—çµæœç¸½å’Œ", f"{sum_weighted:,.2f}")

        st.info(" \n".join([f"- {m}" for m in (read_msgs + msgs)]))

        st.subheader("âœ… ä¸»çµæœæ˜ç´°")
        st.dataframe(final_df, use_container_width=True, height=520)

        safe_download_card(
            "âœ… ä¸‹è¼‰ CSVï¼ˆåŠ æ¬Šè¨ˆç®—çµæœï¼‰",
            to_csv_bytes(final_df),
            "è™•ç†å®Œæˆ_åŠ æ¬Šè¨ˆç®—çµæœ.csv",
            mime="text/csv",
        )

        st.divider()

        with st.expander("âš ï¸ æ¯”å°ä¸åˆ°ã€å¤§é¡åŠ æ¬Šã€çš„æ˜ç´°ï¼ˆå¤§é¡å­˜åœ¨ä½†åŠ æ¬Šå€¼ç¼ºå¤±ï¼‰", expanded=(len(miss_weight_df) > 0)):
            if len(miss_weight_df) == 0:
                st.success("æ²’æœ‰æ¯”å°ä¸åˆ°çš„å¤§é¡ âœ…")
            else:
                cat_summary = (
                    miss_weight_df.assign(å¤§é¡=miss_weight_df["å¤§é¡"].astype(str).str.strip())
                    .groupby("å¤§é¡", dropna=False)
                    .size()
                    .reset_index(name="ç­†æ•¸")
                    .sort_values("ç­†æ•¸", ascending=False)
                )
                st.dataframe(cat_summary, use_container_width=True, height=240)
                st.dataframe(miss_weight_df, use_container_width=True, height=520)
                safe_download_card(
                    "â¬‡ï¸ ä¸‹è¼‰ CSVï¼ˆæ¯”å°ä¸åˆ°å¤§é¡åŠ æ¬Š-æ˜ç´°ï¼‰",
                    to_csv_bytes(miss_weight_df),
                    "æœªå°æ‡‰å¤§é¡åŠ æ¬Š_æ˜ç´°.csv",
                    mime="text/csv",
                )

        with st.expander("âš ï¸ å•†å“æ‰¾ä¸åˆ°ã€å•†å“ä¸»æª”ã€æ˜ç´°ï¼ˆå¤§é¡ç©ºç™½ï¼‰", expanded=False):
            if len(miss_master_df) == 0:
                st.success("å…¨éƒ¨å•†å“çš†èƒ½å°æ‡‰å•†å“ä¸»æª” âœ…")
            else:
                st.dataframe(miss_master_df, use_container_width=True, height=520)
                safe_download_card(
                    "â¬‡ï¸ ä¸‹è¼‰ CSVï¼ˆå•†å“æœªå°æ‡‰ä¸»æª”-æ˜ç´°ï¼‰",
                    to_csv_bytes(miss_master_df),
                    "æœªå°æ‡‰å•†å“ä¸»æª”_æ˜ç´°.csv",
                    mime="text/csv",
                )

        st.success("å®Œæˆ âœ…")

    except Exception as e:
        st.error(f"åŸ·è¡Œä¸­ç™¼ç”Ÿå•é¡Œï¼š{e}")


if __name__ == "__main__":
    main()
